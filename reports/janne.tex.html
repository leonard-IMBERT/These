<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>TeXtidote analysis</title>
<style type="text/css">
body {
  font-family: sans-serif;
}
.highlight, .highlight-sh, .highlight-spelling {
  padding: 2pt;
  border-radius: 4pt;
  cursor: help;
  opacity: 0.7;
  border: dashed 1px;
}
.highlight {
  background-color: orange;
  color: black;
}
.highlight-sh {
  background-color: yellow;
  color: black;
}
.highlight-spelling {
  background-color: red;
  color: white;
}
div.original-file {
  font-family: monospace;
  font-size: 11pt;
  background-color: #f8f8ff;
  padding: 20pt;
  border-radius: 6pt;
}
.textidote {
  	background-image: url(data:image/svg+xml;base64,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);
}
h2.filename {
  font-family: monospace;
}
h1.textidote {
  width: 378px;
  height: 68px;
  display: block;
}
.keyword1 {
  font-weight: bold;
  color: green;
}
.keyword2 {
  font-weight: bold;
  color: darkblue;
}
.comment, .comment * {
  color: darkred;
  font-weight: normal;
}
.linenb {
  font-style: italic;
  color: lightgrey;
  width: 30pt;
  float: left;
  margin-top: 1pt;
  margin-bottom: 1pt;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.codeline {
  margin-left: -30pt;
  padding-left: 60pt;
  margin-top: 1pt;
  margin-bottom: 1pt;
}
.no-text {
  display: none;
}
.clear {
  clear: both;
}
</style>
</head>
<body>
<a href="https://sylvainhalle.github.io/textidote"><h1 class="textidote"><span class="no-text">Results of TeXtidote analysis</span></h1></a>
<p>Here is the result of analyzing your file(s) with TeXtidote. Hover the mouse over highlighted portions of the document to read a tooltip that gives you some writing advice.</p>
<p>Found 20 warning(s)</p>
<div class="original-file">
<div class="linenb">&nbsp;&nbsp;1</div><div class="codeline"><span class="keyword1">\documentclass</span>[../main.tex]{subfiles}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;2</div><div class="codeline">\graphicspath{{\subfix{..}}}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;3</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;4</div><div class="codeline"><span class="keyword2">\begin{document}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;5</div><div class="codeline">\chapter{Reliability of machine learning methods}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;6</div><div class="codeline"><span class="keyword1">\label</span>{sec:janne}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;7</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;8</div><div class="codeline">\epigraph{``Psychohistory was the quintessence of sociology; it was the science of human behavior reduced to mathematical equations. The individual human being is unpredictable, but the reactions of human mobs, Seldon found, could be treated statistically''}{Isaac Asimov, Second Foundation}</div><div class="clear"></div>
<div class="linenb">&nbsp;&nbsp;9</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;10</div><div class="codeline">\minitoc</div><div class="clear"></div>
<div class="linenb">&nbsp;11</div><div class="codeline"><span class="comment"><span class="comment">%</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;12</div><div class="codeline"><span class="comment"><span class="comment">% <span class="keyword1">\textit</span>{In this chapter I discuss the reliability of reconstruction algorithms, especially ML, in JUNO.}</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;13</div><div class="codeline"><span class="comment"><span class="comment">%</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;14</div><div class="codeline"><span class="comment"><span class="comment">% <span class="keyword1">\textit</span>{It is crucial for JUNO not only to reconstruct very precisely the energy of antineutrinos, but also to understand the quality of this reconstruction, and the differences in this between real data and the models assumed by the fits employed to perform the oscillation analysis.}</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;15</div><div class="codeline"><span class="comment"><span class="comment">%</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;16</div><div class="codeline"><span class="comment"><span class="comment">% <span class="keyword1">\textit</span>{We believe the first step in reliability studies is the comparison of the numerous reconstruction algorithms available in JUNO. In this goal, I present the implementation of a BDT for energy reconstruction that was developed by another scientist team in JUNO common software. I compare its resolution and behavior to OMILREC and use the combination method to probe for potential missing information in either of the algorithm}</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;17</div><div class="codeline"><span class="comment"><span class="comment">%</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;18</div><div class="codeline"><span class="comment"><span class="comment">% <span class="keyword1">\textit</span>{We also explore  the usage of an Adversarial Neural Network to produce perturbation in the event measurement (in recorded charge and time of PMT) that would distort the resulting energy spectrum while being invisible to the calibration.}</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;19</div><div class="codeline"><span class="comment"><span class="comment">% <span class="keyword1">\textit</span>{I first discuss the method of this procedure, the Architecture that implement the method, the problematics and the implementation we present in this thesis. I finally present the training and the result obtained}</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;20</div><div class="codeline"><span class="comment"><span class="comment">% <span class="keyword1">\textit</span>{We conclude this chapter by explaining that this first ANN prototype does not manage to generate perturbations that affect IBD events more than control sample events.  However, this exploration taught us several things, among which : it is very difficult to design an ANN able to introduce perturbations at the individual PMT level; some physics-informed guidance will be necessary to obtain an operational tool in the future.}</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;21</div><div class="codeline"><span class="comment"><span class="comment">%</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;22</div><div class="codeline"><span class="comment"><span class="comment">% \rule{\textwidth}{0.4pt}</span></span></div><div class="clear"></div>
<div class="linenb">&nbsp;23</div><div class="codeline">As explained in previous chapters, JUNO is a precision experiment where a very precise understanding of the reconstruction effects is crucial. JUNO is a high-precision experiment, and any discrepancies in the understanding of reconstruction effects can significantly impact the neutrino mass ordering (NMO) determination. Particular attention must be paid to potential biases arising from energy scale miscalibration, charge non-linearity, and differences between real and simulated detector responses, which could skew the oscillation results.</div><div class="clear"></div>
<div class="linenb">&nbsp;24</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;25</div><div class="codeline">While the liquid scintillator technology is well known, this is the first time it is deployed to such scale, and for such precision. This novelty bring makes this task difficult: a lot of effects should ideally be understood better that in previous experiments.</div><div class="clear"></div>
<div class="linenb">&nbsp;26</div><div class="codeline">We already know that a bad knowledge of the energy scale can have consequences as serious as excluding the wrong NMO. It must be known at the 1\% level to reach the desired sensitivity. The present chapter is motivated by a specific question: even very small differences between reconstruction effects in real data and in the models used in oscillation fits could alter this sensitivity or bias the results. Reconstruction algorithms developed in JUNO, in particular those based on ML, try to use the information present in the detector as exhaustively as possible. This might apply even more to future algorithms. There is here a risk that some used information is not similar is real data and in the detector's model, leading to the problem mentioned above. We worked on two ways to address this concern.</div><div class="clear"></div>
<div class="linenb">&nbsp;27</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;28</div><div class="codeline">We think the simpler way to study this reliability issue is to compare algorithm with each other. Differences between various reconstructed spectra of the same sample provide an envelope to evaluate the scale of a potential <span class="highlight" title="This word has been used in one of the immediately preceding sentences. Using a synonym could make your text more interesting to read, unless the repetition is intentional.. Suggestions: [issue, concern, difficulty] (2171) [lt:en:EN_REPEATEDWORDS]">problem</span>. Comparisons on the event per event basis allows showing that 2 algorithms do not use the same information, and is a first step to characterize this difference.</div><div class="clear"></div>
<div class="linenb">&nbsp;29</div><div class="codeline">We already showed a large variety of reconstruction algorithms, OMILREC for LPMT reconstruction in Section \ref{sec:juno:reco}, numerous machine learning algorithms in Section \ref{sec:juno:ml} and our own work in Chapters \ref{sec:jcnn} and \ref{sec:jgnn}. Those algorithms were compared to each other based on their performance as in \cite{qian_vertex_2021} but we are the first that looked into the correlation between the reconstruction. The combinations of algorithms shown in Section \ref{sec:jcnn:combination} show that some information elude the algorithms. To efficiently compare algorithms between each other, they need to be publicly available to the collaboration to studies their differences, event by event.</div><div class="clear"></div>
<div class="linenb">&nbsp;30</div><div class="codeline">Nothing of that kind was possible in JUNO up to now. I imported in JUNO's official software some tools necessary to ML algorithms, and I implemented a first algorithm:  a for energy reconstruction, named BDTE which was developed by Gavrikov et al.\ \cite{gavrikov_energy_2022}, another JUNO's research team. We paved the way for this movement to continue, so that all ML algorithms be available to any JUNO analyst. The details of this implementation and its combination with OMILREC are presented in Section \ref{sec:janne:BDTE}.</div><div class="clear"></div>
<div class="linenb">&nbsp;31</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;32</div><div class="codeline">The other way we have explored to study reliability is to challenge reconstruction algorithms with physically plausible perturbations in the PMT charge and time information. More specifically: these perturbations embody differences between the real detector and its model,  and we want to identify perturbation patterns which  would be too subtle to be detected via data/MC comparisons in calibration data or in control samples from physics data, but which would still be able to alter the oscillation analysis result.  We could try to design such perturbations ``by hand'', based on our knowledge of JUNO. However, with as much as 17600 LPMT, finding subtle perturbations that affecting undefined combinations of PMT would be an endless process.</div><div class="clear"></div>
<div class="linenb">&nbsp;33</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;34</div><div class="codeline">We propose leveraging machine learning by developing an Adversarial Neural Network (ANN) to introduce perturbations reflecting discrepancies between the real detector and its model. However, one challenge with ANN-based approaches is ensuring that the generated perturbations remain physically plausible and are not overly sensitive to random noise or edge effects in the detector.</div><div class="clear"></div>
<div class="linenb">&nbsp;35</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;36</div><div class="codeline">In Section \ref{sec:janne:method}, I describe the method behind the algorithm. In Section \ref{sec:janne:arch} I detail the architecture of our algorithm.</div><div class="clear"></div>
<div class="linenb">&nbsp;37</div><div class="codeline">The training and the results of our method are presented in Section \ref{sec:janne:arch:training}. Finally, in Section \ref{sec:janne:conclusion}, I conclude and discuss the prospects and possible improvements to bring to this work.</div><div class="clear"></div>
<div class="linenb">&nbsp;38</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;39</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;40</div><div class="codeline"><span class="keyword1">\section</span>{First implementation of ML methods in JUNO's software}</div><div class="clear"></div>
<div class="linenb">&nbsp;41</div><div class="codeline"><span class="keyword1">\label</span>{sec:janne:BDTE}</div><div class="clear"></div>
<div class="linenb">&nbsp;42</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;43</div><div class="codeline">To study the reliability of reconstruction algorithms it's necessary to be able to compare their reconstruction performance event by event. To ease the process, it is important that they are publicly available. JUNO's common software, discussed in Section \ref{sec:juno:software}, is based on the SNiPER framework \cite{lin_application_2017} which allows the packaging of the different steps of JUNO's analysis, from Monte Carlo (MC) data generation to event reconstruction, including the propagation and interactions of the particles in the LS, the emission and propagation of the scintillation light, the simulation of the PMTs' waveform reconstruction, electronic effects and the trigger system.</div><div class="clear"></div>
<div class="linenb">&nbsp;44</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;45</div><div class="codeline">This framework is modular, with each module being a C++ class bound in Python. The execution of successive algorithms is orchestrated via Python scripts.</div><div class="clear"></div>
<div class="linenb">&nbsp;46</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;47</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;48</div><div class="codeline">We could have implemented the algorithms presented in Chapters \ref{sec:jcnn} and \ref{sec:jgnn}, but since these are themselves not trivial, we chose to start with a simpler ML algorithm that presents similar energy reconstruction performances as OMILREC: a Boosted Decision Tree (BDT) for energy reconstruction developed by Gavrikov Arsenii et al.\ \cite{gavrikov_energy_2022}.</div><div class="clear"></div>
<div class="linenb">&nbsp;49</div><div class="codeline">This BDT, named BDTE, is based on an aggregated features approach where instead of providing an ML algorithm with low level information, namely the full list of $(Q,t)$ in LPMTs, a set of higher-level variables is designed based on physicist's common knowledge and then fed to the BDT. The list of the aggregated features used by the BDT is presented in Table \ref{tab:janne:bdte:features}. These higher-order variables are extracted from the charge $Q$ and hit time $t$ distribution. It also depends on two straightforward interaction vertex estimators.</div><div class="clear"></div>
<div class="linenb">&nbsp;50</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;51</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;52</div><div class="codeline">The first one is the charge barycenter</div><div class="clear"></div>
<div class="linenb">&nbsp;53</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;54</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{eq:janne:bdte:cc}</div><div class="clear"></div>
<div class="linenb">&nbsp;55</div><div class="codeline">&nbsp;&nbsp;\vec{r}_{cc} = \frac{\sum_i \vec{r}_{PMT,i} Q_i}{\sum_i Q_i}</div><div class="clear"></div>
<div class="linenb">&nbsp;56</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;57</div><div class="codeline">where<span class="highlight" title="The personal pronoun 'I' should be uppercase.. Suggestions: [I] (6634) [lt:en:I_LOWERCASE]"> $i$</span> index the fired PMT, $\vec{r}_{PMT, i}$ is the position vector of the $i$-th PMT and $Q_i$ is the charge it collected.</div><div class="clear"></div>
<div class="linenb">&nbsp;58</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;59</div><div class="codeline">The second estimator is the hit time barycenter</div><div class="clear"></div>
<div class="linenb">&nbsp;60</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;61</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{eq:janne:bdt:cht}</div><div class="clear"></div>
<div class="linenb">&nbsp;62</div><div class="codeline">&nbsp;&nbsp;\vec{r}_{ht} = \frac{1}{\sum_i \frac{1}{t_i + c}} \sum_i \frac{\vec{r}_{PMT, i}}{t_i + c}</div><div class="clear"></div>
<div class="linenb">&nbsp;63</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;64</div><div class="codeline">where $t_i$ is the time of collection of the $i$-th PMT and $c = 50$ ns a constant to prevent divergence when $t_i$ is 0.</div><div class="clear"></div>
<div class="linenb">&nbsp;65</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;66</div><div class="codeline"><span class="keyword2">\begin{table}</span>[ht]</div><div class="clear"></div>
<div class="linenb">&nbsp;67</div><div class="codeline">&nbsp;&nbsp;\centering</div><div class="clear"></div>
<div class="linenb">&nbsp;68</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\begin{tabular}</span>{|l | l|}</div><div class="clear"></div>
<div class="linenb">&nbsp;69</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;\hline</div><div class="clear"></div>
<div class="linenb">&nbsp;70</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;Feature &amp; Description \\</div><div class="clear"></div>
<div class="linenb">&nbsp;71</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;\hline</div><div class="clear"></div>
<div class="linenb">&nbsp;72</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;AccumCharge             &amp; Sum of the charge collected by every LPMT \\</div><div class="clear"></div>
<div class="linenb">&nbsp;73</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;$R_{ht}               $ &amp; Radius reconstructed by the hit time barycenter  \\</div><div class="clear"></div>
<div class="linenb">&nbsp;74</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;$z_{cc}               $ &amp; $z$ component of the vertex reconstructed by the charge barycenter \\</div><div class="clear"></div>
<div class="linenb">&nbsp;75</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;$\sigma \text{PE}     $ &amp; Standard deviation of the distribution of collected PE per PMTs \\</div><div class="clear"></div>
<div class="linenb">&nbsp;76</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;$N_{PMT}              $ &amp; Number of fired PMTs \\</div><div class="clear"></div>
<div class="linenb">&nbsp;77</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;$\text{ht}_{Kurtosis} $ &amp; Kurtosis of the hit time distribution \\</div><div class="clear"></div>
<div class="linenb">&nbsp;78</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;$\text{ht}_{25\%-20\%}$ &amp; Difference between the 25\% and 20\% percentiles of the hit time distribution \\</div><div class="clear"></div>
<div class="linenb">&nbsp;79</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;$R_{cc}               $ &amp; Radius reconstructed by the center of charge barycenter \\</div><div class="clear"></div>
<div class="linenb">&nbsp;80</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;$\text{ht}_{5\%-2\%}  $ &amp; Difference between the 5\% and 2\% percentiles of the hit time distribution \\</div><div class="clear"></div>
<div class="linenb">&nbsp;81</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;$\langle \text{PE} \rangle$ &amp; Mean number of PE collected per PMTs \\</div><div class="clear"></div>
<div class="linenb">&nbsp;82</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;${\cal J}_{ht}        $ &amp; Jacobian of the hit time distrbution \\</div><div class="clear"></div>
<div class="linenb">&nbsp;83</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;$\phi_{cc}            $ &amp; $\phi$ component in spherical coordinate of the charge barycenter \\</div><div class="clear"></div>
<div class="linenb">&nbsp;84</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;$\text{ht}_{35\%-30\%}$ &amp; Difference between the 25\% and 20\% percentiles of the hit time distribution \\</div><div class="clear"></div>
<div class="linenb">&nbsp;85</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;$\text{ht}_{20\%-15\%}$ &amp; Difference between the 20\% and 15\% percentiles of the hit time distribution \\</div><div class="clear"></div>
<div class="linenb">&nbsp;86</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;$\text{PE}_{35\%}     $ &amp; Value of the 35\% percentile of the charge distribution \\</div><div class="clear"></div>
<div class="linenb">&nbsp;87</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;$\text{ht}_{30\%-25\%}$ &amp; Difference between the 30\% and 25\% percentiles of the hit time distribution \\</div><div class="clear"></div>
<div class="linenb">&nbsp;88</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;\hline</div><div class="clear"></div>
<div class="linenb">&nbsp;89</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\end{tabular}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;90</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\caption</span>{Summary of the aggregated features used by the BDT to reconstruct the IBD energy. The charge barycenter and hit time barycenter vertex estimators are detailed in Eq. \ref{eq:janne:bdte:cc} and \ref{eq:janne:bdt:cht} respectively.}</div><div class="clear"></div>
<div class="linenb">&nbsp;91</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{tab:janne:bdte:features}</div><div class="clear"></div>
<div class="linenb">&nbsp;92</div><div class="codeline"><span class="keyword2">\end{table}</span></div><div class="clear"></div>
<div class="linenb">&nbsp;93</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;94</div><div class="codeline">The performance of this BDT, as published by Gavrikov Arsenii et al., is reported in Figure \ref{fig:janne:bdte:orignal_perf}. This BDT is developed in Python using the XGBoost \cite{chen_xgboost_2016} library and originally consisted of a collection of Python scripts for the training and the evaluation.</div><div class="clear"></div>
<div class="linenb">&nbsp;95</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;96</div><div class="codeline">As stated before, JUNO software is composed of C++ modules orchestrated through Python scripts. The technical challenge was to extract the data from the internal representation of the event in JUNO software, the Event Data Model (EDM), into a comprehensible format for Python. This task, which was previously done via data pre-processing by Python scripts, had to be internalized within the software. The computation of the aggregated features was migrated from the Python scripts into C++ modules. The final step was to fetch the reconstruction results of the algorithm into the C++ framework to save the results in the EDM. Some Python libraries were missing, notably XGBoost. A request to the collaboration was issued for the packaging of these libraries with the common software. As a workaround, the documentation of the algorithm contains the procedure to locally install the missing libraries.</div><div class="clear"></div>
<div class="linenb">&nbsp;97</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">&nbsp;98</div><div class="codeline">We validated the consistency of the aggregated features between the original Python implementation and the JUNO software by comparing 1,000 events with the help of Arsenii. For the majority of the features, the relative difference between his and ours was either 0 or of the order of $10^{-15}$, except features: $R_{cc}$, $R_{ht}$, and $z_{cc}$. For these three features, the relative difference is about $10^{-6}$, which, while small, is still surprisingly high for numerical computation. The distributions of the relative differences for these features are presented in Figure \ref{fig:janne:feat_diff}.</div><div class="clear"></div>
<div class="linenb">&nbsp;99</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">100</div><div class="codeline">We investigated the source of these discrepancies. The difference in computation environments -- Python using Numpy \cite{harris_array_2020} and C++ using the standard library in our case -- is most likely the cause. Since the discrepancies arise from the computation of the barycenter in Eq. \ref{eq:janne:bdte:cc} and \ref{eq:janne:bdt:cht}, they may result from differences in compiler optimization during the weighted sum calculation. We consider that these differences are still small enough that the performance of the BDT is unaffected.</div><div class="clear"></div>
<div class="linenb">101</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">102</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">103</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[ht]</div><div class="clear"></div>
<div class="linenb">104</div><div class="codeline">&nbsp;&nbsp;\centering</div><div class="clear"></div>
<div class="linenb">105</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\begin{subfigure}</span>[t]{0.32\linewidth}</div><div class="clear"></div>
<div class="linenb">106</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[width=\linewidth]{images/janne/bdte/R_cc_diff.png}</div><div class="clear"></div>
<div class="linenb">107</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\end{subfigure}</span></div><div class="clear"></div>
<div class="linenb">108</div><div class="codeline">&nbsp;&nbsp;\hfill</div><div class="clear"></div>
<div class="linenb">109</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\begin{subfigure}</span>[t]{0.32\linewidth}</div><div class="clear"></div>
<div class="linenb">110</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[width=\linewidth]{images/janne/bdte/R_cht_diff.png}</div><div class="clear"></div>
<div class="linenb">111</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\end{subfigure}</span></div><div class="clear"></div>
<div class="linenb">112</div><div class="codeline">&nbsp;&nbsp;\hfill</div><div class="clear"></div>
<div class="linenb">113</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\begin{subfigure}</span>[t]{0.32\linewidth}</div><div class="clear"></div>
<div class="linenb">114</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[width=\linewidth]{images/janne/bdte/z_cc_diff.png}</div><div class="clear"></div>
<div class="linenb">115</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\end{subfigure}</span></div><div class="clear"></div>
<div class="linenb">116</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\caption</span>{Relative difference between the features computed by Gavrikov et al.\ (superscripted Paper) and our implementation (superscripted Implementation).}</div><div class="clear"></div>
<div class="linenb">117</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{fig:janne:feat_diff}</div><div class="clear"></div>
<div class="linenb">118</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">119</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">120</div><div class="codeline">The performance of our implementation of BDTE compared to the results presented in \cite{gavrikov_energy_2022} are presented in Figure \ref{fig:janne:bdte:implementation_perf}.</div><div class="clear"></div>
<div class="linenb">121</div><div class="codeline"><span class="keyword2">\begin{itemize}</span></div><div class="clear"></div>
<div class="linenb">122</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\item</span> At 1 MeV, the relative resolution is reported by the publication is just below 3\%/MeV. Our implementation show a relative resolution of 2.8\%. The relative bias is reported at -0.1\%, same as for our implementation.</div><div class="clear"></div>
<div class="linenb">123</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\item</span> At 4 MeV, the reported relative resolution is 1.5\%, our implementation show 1.45\%. The relative bias is about 0.05\% in both results.</div><div class="clear"></div>
<div class="linenb">124</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\item</span> At 10 MeV, our implementation reconstruct the energy with a resolution of 1\% whereas the publication report a resolution a bit greater than 1\%. They report a positive relative bias of 0.05\% while we see a negative 0.1\%. This difference might come from the fact that Arsenii provided us an updated version of the BDT since the publication of \cite{gavrikov_energy_2022}.</div><div class="clear"></div>
<div class="linenb">125</div><div class="codeline"><span class="keyword2">\end{itemize}</span></div><div class="clear"></div>
<div class="linenb">126</div><div class="codeline">The performance are considered compatibles.</div><div class="clear"></div>
<div class="linenb">127</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">128</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[ht]</div><div class="clear"></div>
<div class="linenb">129</div><div class="codeline">&nbsp;&nbsp;\centering</div><div class="clear"></div>
<div class="linenb">130</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\begin{subfigure}</span>[t]{0.58\linewidth}</div><div class="clear"></div>
<div class="linenb">131</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[width=\linewidth]{images/janne/bdte/bdte_perf.png}</div><div class="clear"></div>
<div class="linenb">132</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;<span class="highlight-sh" title="A caption should end with a period [sh:capperiod]"><span class="keyword1">\caption</span>{}</span></div><div class="clear"></div>
<div class="linenb">133</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword1">\label</span>{fig:janne:bdte:orignal_perf}</div><div class="clear"></div>
<div class="linenb">134</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\end{subfigure}</span></div><div class="clear"></div>
<div class="linenb">135</div><div class="codeline">&nbsp;&nbsp;\hfill</div><div class="clear"></div>
<div class="linenb">136</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\begin{subfigure}</span>[t]{0.38\linewidth}</div><div class="clear"></div>
<div class="linenb">137</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[width=\linewidth]{images/janne/bdte/e_rec_vs_e_true.png}</div><div class="clear"></div>
<div class="linenb">138</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;<span class="highlight-sh" title="A caption should end with a period [sh:capperiod]"><span class="keyword1">\caption</span>{}</span></div><div class="clear"></div>
<div class="linenb">139</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword1">\label</span>{fig:janne:bdte:implementation_perf}</div><div class="clear"></div>
<div class="linenb">140</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\end{subfigure}</span></div><div class="clear"></div>
<div class="linenb">141</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\caption</span>{Resolution of BDTE <span class="keyword1">\textbf</span>{On the left: } as reported by Gavrikov Arsenii et. al in \cite{gavrikov_energy_2022}, <span class="keyword1">\textbf</span>{On the right: } once implemented in JUNO common software. On the right plot is also reported the reconstruction performance of the OMILREC algorithm. The OMILREC algorithm $E_{vis}$ has been corrected to $E_{dep}$ following the procedure detailled in Annex \ref{sec:annex:evis}.}</div><div class="clear"></div>
<div class="linenb">142</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{fig:janne:bdte:perf}</div><div class="clear"></div>
<div class="linenb">143</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">144</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">145</div><div class="codeline">The reconstruction using BDTE was implemented in JUNO's common software but Gavrikov et al.\ also detail the training and hyper-optimization. JUNO Monte Carlo is likely to evolve during the construction phase and will be further adjusted using calibration. The implementation of those procedures, the training and optimization, will be required as BDTE re-training and re-optimization will be required with each JUNO software update.</div><div class="clear"></div>
<div class="linenb">146</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">147</div><div class="codeline">Figure \ref{fig:janne:bdte:implementation_perf} shows that the resolution of BDTE is very close to OMILREC. We measured the correlation between their reconstructions, focusing on the residuals with respect to the common true deposited energy:</div><div class="clear"></div>
<div class="linenb">148</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">149</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{eq:janne:bdte:corr}</div><div class="clear"></div>
<div class="linenb">150</div><div class="codeline">\mathrm{Corr}(E_{BDTE} - E_{dep}, E_{OMILREC} - E_{dep})</div><div class="clear"></div>
<div class="linenb">151</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">152</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">153</div><div class="codeline">If the correlation is small enough, it indicates that these two reconstruction algorithms do not use the same information. As a corollary, it indicates that these algorithms can in principle be improved. The correlation between errors for different energy and event radius in the detector is presented in Figure \ref{fig:janne:bdte:corr}. We see that for the vast majority of the $(R^3, E)$ phase space, the correlation is &gt; 0.995, down to $\sim$ 0.98 in the $R \approx 9$ m and $R &gt; 17$ m regions. Such high correlations indicate that these algorithms are very close to using the same information. No difference can be found here, that could be used to improve them. Maybe the situation will be different when other ML algorithms are implemented in JUNO's software and the same exercise carried out. The fact that  BDTE and OMILREC are so correlated, and their performance so similar, is interesting: it suggests that using the full $(Q, t)$ list as inputs does not allow major improvements. This is in line with some conclusions we expressed in previous chapters: to improve  JUNO's reconstruction by starting from low level variables might require to use rawer variables than $(Q,t)$, like the full waveforms.</div><div class="clear"></div>
<div class="linenb">154</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">155</div><div class="codeline"><span class="keyword2">\begin{figure}</span></div><div class="clear"></div>
<div class="linenb">156</div><div class="codeline">&nbsp;&nbsp;\centering</div><div class="clear"></div>
<div class="linenb">157</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[height=7cm]{images/janne/bdte/corr_bdte_omilrec.png}</div><div class="clear"></div>
<div class="linenb">158</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\caption</span>{Correlation between the errors in energy reconstruction between BDTE and OMILREC (Eq. \ref{eq:janne:bdte:corr}). The correlation is computed in $R^3$ bins of 216 m$^3$ between 0 and 5000 m$^3$, 0 and 17 m in y axis, and in 0.40 MeV bins between 1.022 and 10.022 MeV of deposited energy.}</div><div class="clear"></div>
<div class="linenb">159</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{fig:janne:bdte:corr}</div><div class="clear"></div>
<div class="linenb">160</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">161</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">162</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">163</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">164</div><div class="codeline"><span class="keyword1">\section</span>{Adversarial method}</div><div class="clear"></div>
<div class="linenb">165</div><div class="codeline"><span class="keyword1">\label</span>{sec:janne:method}</div><div class="clear"></div>
<div class="linenb">166</div><div class="codeline"><span class="comment"><span class="comment">%<span class="keyword2">\begin{itemize}</span></span></span></div><div class="clear"></div>
<div class="linenb">167</div><div class="codeline"><span class="comment"><span class="comment">%  <span class="keyword1">\item</span> JUNO needs very good understanding of reconstruction</span></span></div><div class="clear"></div>
<div class="linenb">168</div><div class="codeline"><span class="comment"><span class="comment">%  <span class="keyword1">\item</span> Estimator combination shows that there can be improvement due to simplfication and that NN/reco methods can have hard time grasping all the detector effect.</span></span></div><div class="clear"></div>
<div class="linenb">169</div><div class="codeline"><span class="comment"><span class="comment">%  <span class="keyword1">\item</span> If there is potential failure point, we need to search for them</span></span></div><div class="clear"></div>
<div class="linenb">170</div><div class="codeline"><span class="comment"><span class="comment">%  <span class="keyword1">\item</span> La mesure de la NMO est tres sensible (see $alpha_{qnl}$ joint fit chapter)</span></span></div><div class="clear"></div>
<div class="linenb">171</div><div class="codeline"><span class="comment"><span class="comment">%<span class="keyword2">\end{itemize}</span></span></span></div><div class="clear"></div>
<div class="linenb">172</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">173</div><div class="codeline">As introduced at the beginning of the chapter, JUNO needs a very good understanding of the biases and effects affecting its reconstruction, and small discrepancies between the real detector and its model could be an issue, in particular when ML algorithms are used. Calibration data will be used to study reconstruction effects and to tune the simulation so that the detector model matches as well as possible the real one. JUNO relies on multiple sources that can be deployed at various positions in the detector. The calibration strategy is already discussed in Section \ref{sec:juno:calib} and shows calibration sources of gammas, neutrons, and positrons (Table \ref{tab:juno:calib_source}), with the catch that the positrons will annihilate inside the encapsulation and only the two 511 keV gammas will deposit energy in the LS.</div><div class="clear"></div>
<div class="linenb">174</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">175</div><div class="codeline">None of the calibration sources considered are positron events. While electrons and positron events should be pretty similar in their interaction with the electronic cloud of the LS atoms,</div><div class="clear"></div>
<div class="linenb">176</div><div class="codeline">electron events are missing the two annihilation gammas. The topology of the event is therefore not the same: electron events display a single interaction site, up to a few cm longs, where a few MeV is deposited; positron events display a similar main site, accompanied by several others low energy (&lt; 300 keV) sites, typically spread over more than 20 cm, due to the Compton interactions of the annihilation gammas. Other differences appear due to the fraction of the positrons that will  form a positronium: it causes a delay of a few nanoseconds between  energy deposition and the positronium annihilation, to be compared to the PMT  transit time spread between 3 and 6 ns, depending on the PMT type \cite{rodphai_20-inch_2021, liao_study_2017, li_characterization_2018}. Therefore, subtle effects might be present in positron events that the analysis of electron events samples cannot capture. Moreover, not all positions can be reached by calibration sources (effects affecting events close to the border of the detector can't be studied perfectly then), and calibration run are punctual in time, and therefore can't witness finely of time evolutions in the detector.</div><div class="clear"></div>
<div class="linenb">177</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">178</div><div class="codeline">The two last issues presented above do not alter a natural source of calibration such as $^{12}B$ events. The $^{12}B$ is a cosmogenically produced isotope through the passage of muons inside the LS. The $^{12}B$ decays via $\beta^-$ emissions with a Q value of 13.5 MeV, with more than 98\% of the decay resulting in ground state $^{12}C$. This results into the e- energy spectrum shown on Fig. \ref{fig:juno:nl:boron}. The energy regime involved here is similar to that of reactor IBDs. The $^{12}B$ events will be cleanly identified by looking for delayed high-energy $\beta$ events after an energetic muon. The $^{12}B$ events will be uniformly distributed in the detector: moreover, $^{12}B$ events are produced continuously, and therefore can be used to follow finally time variations in the detector behavior. As with calibration sources, energy spectra obtained with measured and simulated $^{12}B$ events can be compared to control the accuracy of the detector model. This <span class="keyword1">\textit</span>{physics control sample} still presents the disadvantage to be an electron source.</div><div class="clear"></div>
<div class="linenb">179</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">180</div><div class="codeline">The limitations of the calibration and control samples mentioned above could hide subtle data/MC discrepancies that might be able to bias the results of the oscillation analysis. We fear this problem in particular when ML algorithms are used, due to their ability to use exhaustively the information present in the detector.</div><div class="clear"></div>
<div class="linenb">181</div><div class="codeline">But, while we have an idea of where the issues could come from, the manual production of event perturbations that go unseen when using these samples would be very time-consuming. That's why we propose to use the power of ML for an automated generation of adequate perturbation scenarios. We choose to develop an Adversarial Neural Network (ANN) to produce those perturbations if they exist. A schematic of the concept is presented in Figure \ref{fig:janne:method:schema}. We try here to extend to a large scale detector the concept introduced in \cite{nachman_ai_2019}.</div><div class="clear"></div>
<div class="linenb">182</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">183</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[ht]</div><div class="clear"></div>
<div class="linenb">184</div><div class="codeline">&nbsp;&nbsp;\centering</div><div class="clear"></div>
<div class="linenb">185</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[width=\linewidth]{images/janne/ann_method.png}</div><div class="clear"></div>
<div class="linenb">186</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\caption</span>{Schema of the method to discover vulnerabilities in the reconstruction methods. <span class="keyword1">\textbf</span>{On the top} of the image, the standard data flow. The individual charge and times are fed to a reconstruction algorithm. From the reconstructed energies, we can produce an IBD spectrum and compute control observables from the calibration and/or control samples (like a $^{12}B$ sample). In an ideal case, these observables should include the energy spectrum, the interaction position distributions, and any other useful variables. On the sketch above, the yellow distribution represent a real data sample while the blue points represents a simulated sample. <span class="keyword1">\textbf</span>{On the bottom}, the same data flow but we add an ANN between the input and the reconstruction. Still in an ideal case, the ANN learns what slight change to impose to each and every PMT so that the input charge and time so the reconstruction algorithm inaccurately reconstruct the IBD energy, but the perturbation is not visible in the control samples.}</div><div class="clear"></div>
<div class="linenb">187</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{fig:janne:method:schema}</div><div class="clear"></div>
<div class="linenb">188</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">189</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">190</div><div class="codeline">This network should produce physically plausible perturbations that would not be seen by the calibration system but also by the visualization of the event. If the ANN manages to produce such perturbations, we can derive systematic uncertainties from it. If it fails to find any, it is a proof of robustness for the targeted reconstruction method.</div><div class="clear"></div>
<div class="linenb">191</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">192</div><div class="codeline"><span class="keyword1">\subsection</span>{ANN Architecture}</div><div class="clear"></div>
<div class="linenb">193</div><div class="codeline"><span class="keyword1">\label</span>{sec:janne:arch}</div><div class="clear"></div>
<div class="linenb">194</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">195</div><div class="codeline">For this study, we consider a ``physics'' dataset composed of 1M positron events from J23, uniformly distributed in the Central Detector (CD) and in deposited energy between $E_{dep} \in [1.022; 10.022]$. This set represents the IBD events we want to the reconstruction to be fooled on.</div><div class="clear"></div>
<div class="linenb">196</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">197</div><div class="codeline">We use a second ``control'' dataset of 1M electron events from J23, also uniformly distributed in the detector and over the same energy range. They mimic the energy deposition of $^{12}B$ decay and are used as the sample to compute the control observables.</div><div class="clear"></div>
<div class="linenb">198</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">199</div><div class="codeline">This work is a collaboration with an engineer from Subatech Gilles Grasseau. We, the JUNO's Subatech group, developed the idea and the global method design and the result interpretation. Gilles and I developed the ANN architecture, and Gilles the FFNN architecture. Gilles was in charge of the Python implementation and I provided the MC samples and readers to read the ROOT data into Python.</div><div class="clear"></div>
<div class="linenb">200</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">201</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">202</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">203</div><div class="codeline"><span class="comment"><span class="comment">%<span class="keyword2">\begin{itemize}</span></span></span></div><div class="clear"></div>
<div class="linenb">204</div><div class="codeline"><span class="comment"><span class="comment">%  <span class="keyword1">\item</span> Expliquer la problematique dans l'architecture</span></span></div><div class="clear"></div>
<div class="linenb">205</div><div class="codeline"><span class="comment"><span class="comment">%  <span class="keyword1">\item</span> Ambition de pouvoir etre appliqué a toutes les methodes, pas que NN</span></span></div><div class="clear"></div>
<div class="linenb">206</div><div class="codeline"><span class="comment"><span class="comment">%  <span class="keyword1">\item</span> Pb techique: descente de gradient</span></span></div><div class="clear"></div>
<div class="linenb">207</div><div class="codeline"><span class="comment"><span class="comment">%  <span class="keyword1">\item</span> Présenter la loss</span></span></div><div class="clear"></div>
<div class="linenb">208</div><div class="codeline"><span class="comment"><span class="comment">%<span class="keyword2">\end{itemize}</span></span></span></div><div class="clear"></div>
<div class="linenb">209</div><div class="codeline">We can describe the goal of the ANN by using following loss function:</div><div class="clear"></div>
<div class="linenb">210</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">211</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{eq:janne:loss}</div><div class="clear"></div>
<div class="linenb">212</div><div class="codeline">&nbsp;&nbsp;\mathcal{L} = \mathcal{L}_{adv} + \mathcal{L}_{reg}</div><div class="clear"></div>
<div class="linenb">213</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">214</div><div class="codeline">where $\mathcal{L}_{adv}$ is the adversarial loss, which is minimal when the reconstruction is ``broken'', i.e.\ when the delta perturbations introduced on Fig \ref{fig:janne:method:schema} are at work. We thus need to define what is a <span class="keyword1">\textit</span>{wrong} reconstruction. <span class="highlight" title="Three successive sentences begin with the same word. Consider rewording the sentence or use a thesaurus to find a synonym.. Suggestions: [Furthermore, we, Likewise, we, Not only that, but we] (17216) [lt:en:ENGLISH_WORD_REPEAT_BEGINNING_RULE]">We</span> choose to define it through the correlation between the reconstructed and deposited energy</div><div class="clear"></div>
<div class="linenb">215</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">216</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{eq:janne:ladv}</div><div class="clear"></div>
<div class="linenb">217</div><div class="codeline">&nbsp;&nbsp;\mathcal{L}_{adv} = |\mathrm{Corr}(E_{rec}, E_{dep})|</div><div class="clear"></div>
<div class="linenb">218</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">219</div><div class="codeline">where $E_{rec}$ and $E_{dep}$ are the reconstructed energy and the true deposited energy respectively.</div><div class="clear"></div>
<div class="linenb">220</div><div class="codeline">This loss is positive or null and is minimal when the reconstructed energy after perturbation is decorrelated with the true deposited energy. If this loss is below 1, there is a chance that the delta perturbations imposed on each PMT alter the result of the oscillation analysis. This loss is evaluated on the physics dataset.</div><div class="clear"></div>
<div class="linenb">221</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">222</div><div class="codeline">The term $\mathcal{L}_{reg}$ is the regularization term, which is minimal when the control variables are correctly reconstructed</div><div class="clear"></div>
<div class="linenb">223</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">224</div><div class="codeline">&nbsp;&nbsp;\mathcal{L}_{reg} = \sum_\lambda (O^{rec}_\lambda - O^{th}_\lambda)^2</div><div class="clear"></div>
<div class="linenb">225</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">226</div><div class="codeline">where $\lambda$ index the different control observables that will be considered in this study. It's minimal when the control observables after perturbation $O^{rec}_\lambda$ are coherent with their expected values $O^{th}_\lambda$.</div><div class="clear"></div>
<div class="linenb">227</div><div class="codeline">In this exploratory work, we pick as the control observable the difference between the reconstructed position and energy and the ground truth from the Monte Carlo simulation. When this loss is minimal, there is a chance that the perturbations will not be seen in data/MC  studies using this control sample, which is one of the goals of the algorithm.</div><div class="clear"></div>
<div class="linenb">228</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">229</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{eq:janne:lreg}</div><div class="clear"></div>
<div class="linenb">230</div><div class="codeline">&nbsp;&nbsp;\mathcal{L}_{reg} = \sum_{\lambda \in \{x, y, z, E\}} (\lambda_{rec} - \lambda_{true})^2</div><div class="clear"></div>
<div class="linenb">231</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">232</div><div class="codeline">This loss is evaluated on the control dataset.</div><div class="clear"></div>
<div class="linenb">233</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">234</div><div class="codeline">To these two loss, we adjoin a penalty term $P$</div><div class="clear"></div>
<div class="linenb">235</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">236</div><div class="codeline">&nbsp;&nbsp;{\cal L} = {\cal L}_{adv} + {\cal L}_{reg} + P</div><div class="clear"></div>
<div class="linenb">237</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">238</div><div class="codeline">This penalty $P$ is here to prevent the ANN from producing event too different from the initial event. It will be further detailed in Section \ref{sec:janne:arch:ann}. This loss is evaluated on both datasets.</div><div class="clear"></div>
<div class="linenb">239</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">240</div><div class="codeline">We see that the final loss is an equilibrium between the adversarial and regularization loss.</div><div class="clear"></div>
<div class="linenb">241</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">242</div><div class="codeline"><span class="keyword1">\subsection</span>{Back-propagation problematic}</div><div class="clear"></div>
<div class="linenb">243</div><div class="codeline"><span class="keyword1">\label</span>{sec:janne:back_prop}</div><div class="clear"></div>
<div class="linenb">244</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">245</div><div class="codeline">We would like this method to be applicable to any kind of reconstruction algorithm, but this is complicated considering standard training method through backward-propagation (method discussed in details in Section \ref{sec:ml:optim}) for reasons developed in this section. This force use to develop, in this exploratory work, a new NN for reconstruction. This NN is presented in Section \ref{sec:janne:arch:reco}.</div><div class="clear"></div>
<div class="linenb">246</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">247</div><div class="codeline">For explanation, let's define the application of the reconstruction algorithm as $\mathcal{F}$ on an event $X$, resulting in the prediction $Y$, and the application of the ANN $\mathcal{G}$ on $X$ to give a perturbed event $X'$. We can parametrize the equation \ref{eq:janne:loss}</div><div class="clear"></div>
<div class="linenb">248</div><div class="codeline"><span class="keyword2">\begin{align}</span></div><div class="clear"></div>
<div class="linenb">249</div><div class="codeline">&nbsp;&nbsp;Y = \mathcal{F}(X); ~~&amp; Y' = \mathcal{F}(X') = \mathcal{F}(\mathcal{G}(X))</div><div class="clear"></div>
<div class="linenb">250</div><div class="codeline"><span class="keyword2">\end{align}</span></div><div class="clear"></div>
<div class="linenb">251</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">252</div><div class="codeline">&nbsp;&nbsp;\mathcal{L} \equiv \mathcal{L}(\mathcal{F}(\mathcal{G}(X)), Y_t)</div><div class="clear"></div>
<div class="linenb">253</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">254</div><div class="codeline">where $Y_t$ is the reconstruction target of $Y$.</div><div class="clear"></div>
<div class="linenb">255</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">256</div><div class="codeline">Now if we consider the learnable parameters $\bm{\theta}$ of the ANN on which we want to optimize $\mathcal{L}$, in the backward-propagation optimization framework we need to compute</div><div class="clear"></div>
<div class="linenb">257</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">258</div><div class="codeline">&nbsp;&nbsp;\frac{\partial \mathcal{L}(\mathcal{F}(\mathcal{G}(X)))}{\partial \bm{\theta}}</div><div class="clear"></div>
<div class="linenb">259</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">260</div><div class="codeline">which, when using the chain rule, become</div><div class="clear"></div>
<div class="linenb">261</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">262</div><div class="codeline">&nbsp;&nbsp;\frac{\partial \mathcal{L}(\mathcal{F}(\mathcal{G}(X)))}{\partial \bm{\theta}} = \frac{\partial \mathcal{G}}{\partial \bm{\theta}} \cdot \frac{\partial \mathcal{F}}{\partial \mathcal{G}} \cdot \frac{\partial \mathcal{L}}{\partial \mathcal{F}}</div><div class="clear"></div>
<div class="linenb">263</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">264</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">265</div><div class="codeline">The terms $\frac{\partial \mathcal{G}}{\partial \bm{\theta}}$ and $\frac{\partial \mathcal{L}}{\partial \mathcal{F}}$ are easily computable but $\frac{\partial \mathcal{F}}{\partial \mathcal{G}}$ depends on the nature of the reconstruction algorithm.</div><div class="clear"></div>
<div class="linenb">266</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">267</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">268</div><div class="codeline">While this term comes naturally when using neural network algorithms, its computation is embedded in most of modern framework, it's not so trivial for other types of algorithms like likelihood. Solutions exist to optimize networks that work in complex, non-differentiable environments, such as <span class="keyword1">\textit</span>{Deep Reinforcement Learning} \cite{kiran_deep_2021, vinyals_grandmaster_2019}, but as a first prototype, we will restrict ourselves to neural networks for the reconstruction algorithm.</div><div class="clear"></div>
<div class="linenb">269</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">270</div><div class="codeline">The choice to use gradient descent, and therefore neural networks, also allowed us to keep all technical software development wrapped in the same language and framework, PyTorch \cite{ansel_pytorch_2024}.</div><div class="clear"></div>
<div class="linenb">271</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">272</div><div class="codeline">The backward-propagation introduce a second issue. At the beginning of the subsection we introduce $X' = \mathcal{G}(X)$, the event after perturbation. It's an input of the reconstruction $\mathcal{F}$, thus, let's say that the event, in its form $X$, is a list of tuples $(id, Q, t)$ which are the hit on the PMT<span class="highlight" title="This abbreviation for 'identification' is spelled all-uppercase.. Suggestions: [ID] (20624) [lt:en:ID_CASING]"> $id$</span>. If $\mathcal{F}$ require the information to be formatted in a specific way (graph, images, $\ldots$) via an algorithm $\tau(X)$, it means that</div><div class="clear"></div>
<div class="linenb">273</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">274</div><div class="codeline">&nbsp;&nbsp;\frac{\partial \mathcal{L}(\mathcal{F}(\tau(\mathcal{G}(X))))}{\partial \bm{\theta}} = \frac{\partial \mathcal{G}}{\partial \bm{\theta}} \cdot \frac{\partial \tau}{\partial \mathcal{G}} \cdot \frac{\partial \mathcal{F}}{\partial \tau} \cdot \frac{\partial \mathcal{L}}{\partial \mathcal{F}}</div><div class="clear"></div>
<div class="linenb">275</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">276</div><div class="codeline">which also requires that $\frac{\partial \tau}{\partial \mathcal{G}}$ is differentiable.</div><div class="clear"></div>
<div class="linenb">277</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">278</div><div class="codeline">On the other hand, if $X$ is already formatted as the input of ${\cal F}$, it means that ${\cal G}$ takes the same format as input, and we drop the requirement on $\tau$ to be differentiable. Specifically, if ${\cal F}$ takes an image as input, it means that ${\cal G}$ will also take an image as input and output an image. Unfortunately, this also means that if some information is lost before ${\cal G}$, for example, during the charge and time aggregation in pixels, the ANN cannot retrieve and modify it.</div><div class="clear"></div>
<div class="linenb">279</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">280</div><div class="codeline">A more elegant solution would that $\mathcal{G}$ would also compute the transformation $\tau$ in addition to finding relevant perturbation, but for the simplicity of this exploratory work, we use <span class="highlight" title="Use 'an' instead of 'a' if the following word starts with a vowel sound, e.g. 'an article', 'an hour'.. Suggestions: [an] (21428) [lt:en:EN_A_VS_AN]">a</span> $\mathcal{G}$ that process transformed data.</div><div class="clear"></div>
<div class="linenb">281</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">282</div><div class="codeline">\<span class="highlight-sh" title="If a section has sub-sections, it should have more than one such sub-section. [sh:nsubdiv]">subsection</span>{Reconstruction Network (FFNN)}</div><div class="clear"></div>
<div class="linenb">283</div><div class="codeline"><span class="keyword1">\label</span>{sec:janne:arch:reco}</div><div class="clear"></div>
<div class="linenb">284</div><div class="codeline"><span class="comment"><span class="comment">%<span class="keyword2">\begin{itemize}</span></span></span></div><div class="clear"></div>
<div class="linenb">285</div><div class="codeline"><span class="comment"><span class="comment">%  <span class="keyword1">\item</span> Reseau de Neurone Simple. Deux avantages:</span></span></div><div class="clear"></div>
<div class="linenb">286</div><div class="codeline"><span class="comment"><span class="comment">%  <span class="keyword1">\item</span> Besoin pour la descente de gradient</span></span></div><div class="clear"></div>
<div class="linenb">287</div><div class="codeline"><span class="comment"><span class="comment">%  <span class="keyword1">\item</span> Un reseau "simpliste" a plus de chance de présenter des "défauts" que l'ANN pourrait exploiter</span></span></div><div class="clear"></div>
<div class="linenb">288</div><div class="codeline"><span class="comment"><span class="comment">%<span class="keyword2">\end{itemize}</span></span></span></div><div class="clear"></div>
<div class="linenb">289</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">290</div><div class="codeline">As introduced just before, we need a NN algorithm for IBD reconstruction. We could have used the GNN presented in Chapter \ref{sec:jgnn}, but we preferred a simpler approach to not be constrained by the memory consumption of the reconstruction network. The memory issue does not really <span class="highlight" title="The auxiliary verb 'do' requires the base form of the verb.. Suggestions: [come] (21769) [lt:en:DID_BASEFORM]">comes</span> from the reconstruction network but from the ANN. The requirement to produce outputs that have the same structure and complexity as the reconstruction network makes it even more memory consuming than the reconstruction network, thus the choice for a simpler reconstruction network. This network is designated as FFNN for ``F''-Fully connected Neural Network where ``F'' is reminder to the ${\cal F}$ from previous section.</div><div class="clear"></div>
<div class="linenb">291</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">292</div><div class="codeline">This network takes as input a vector containing the results of the aggregation of charge and time on pixels, forming a vectorized image. We consider JUNO to be composed of 3072 pixels defined by the HealPix \cite{gorski_healpix_2005} pixelization. On each of these pixels, we sum the charges and keep the first time of hit, resulting in 3072 $(Q,t)$ tuples. To these tuples, we adjoin the position of the center of these pixels, resulting in 3072 $(Q,t,x,y,z)$ tuples. The data is finally represented as <span class="highlight" title="Use 'an' instead of 'a' if the following word starts with a vowel sound, e.g. 'an article', 'an hour'.. Suggestions: [an] (22653) [lt:en:EN_A_VS_AN]">a</span> $3072 \times 5 = 15360$ vector. In the case where the charge in a pixel is 0, the time is set to 2048 ns, which is way after the closure of the trigger window.</div><div class="clear"></div>
<div class="linenb">293</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">294</div><div class="codeline">The charge is expressed in $N_{pe}$ and the time of hit in nanoseconds. The time is negative, meaning that 0 ns the first hit time and -2048 ns is the latest hit time.</div><div class="clear"></div>
<div class="linenb">295</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">296</div><div class="codeline">FFNN is a Fully Connected Neural Network (FCDNN) composed of the following layers: the input layer, providing the 15360-item vector, followed by fully connected linear layers with the respective number of neurons being $[8192,4096,2048,1024,512,256,128,64,32]$. These layers possess a Leaky ReLU activation function defined as</div><div class="clear"></div>
<div class="linenb">297</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">298</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">299</div><div class="codeline">&nbsp;&nbsp;\mathrm{LeakyReLU} = <span class="keyword2">\begin{cases}</span></div><div class="clear"></div>
<div class="linenb">300</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;x, &amp; \text{if } x &gt; 0 \\</div><div class="clear"></div>
<div class="linenb">301</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;10^{-2} \cdot x, &amp; \text{otherwise}</div><div class="clear"></div>
<div class="linenb">302</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\end{cases}</span></div><div class="clear"></div>
<div class="linenb">303</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">304</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">305</div><div class="codeline">The last layer is a linear layer with 4 neurons, representing $(x,y,z,E)$ without an activation function.</div><div class="clear"></div>
<div class="linenb">306</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">307</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">308</div><div class="codeline">The loss used is the Mean Square Error (MSE)</div><div class="clear"></div>
<div class="linenb">309</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">310</div><div class="codeline">&nbsp;&nbsp;\text{MSE}(\bm{\eta}, \bm{\eta}^{true}) = \sum_i (\eta_i - \eta_i^{true})^2</div><div class="clear"></div>
<div class="linenb">311</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">312</div><div class="codeline">where $\eta$ takes the values of $(x, y, z, E)$.</div><div class="clear"></div>
<div class="linenb">313</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">314</div><div class="codeline">The optimizer used for its training is the Stochastic Gradient Descent with momentum</div><div class="clear"></div>
<div class="linenb">315</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">316</div><div class="codeline">&nbsp;&nbsp;\bm{\theta}_{t+1} = \bm{\theta}_t - \Lambda \left(\sum_{i=0} \frac{\partial \mathcal{L}}{\partial \bm{\theta}_{t - i}} \cdot 0.9^{i} \right)</div><div class="clear"></div>
<div class="linenb">317</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">318</div><div class="codeline">where $\bm{\theta}_t$ is vector of learnable parameters at step $t$. $\Lambda$ is the learning rate set at  $10^{-3}$. The difference with the classical SGD is the gradient term with $i &gt; 1$. We save the gradient computed in the previous step and use them as momentum with a decaying weight. The factor 0.9 is a hyperparameter that has been selected for the training.</div><div class="clear"></div>
<div class="linenb">319</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">320</div><div class="codeline">Additionally, to prevent over-fitting, we introduce a weight decay. Each step, we reduce the amplitude of the parameters $\bm{\theta}$ by $10^{-3}$:</div><div class="clear"></div>
<div class="linenb">321</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">322</div><div class="codeline">&nbsp;&nbsp;\bm{\theta}_{t+1} = \bm{\theta}_t \cdot (1 - 10^{-3})</div><div class="clear"></div>
<div class="linenb">323</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">324</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">325</div><div class="codeline"><span class="keyword1">\subsubsection</span>{Performances}</div><div class="clear"></div>
<div class="linenb">326</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">327</div><div class="codeline">The FFNN is trained independently of the ANN. The dataset is composed of 1M positrons events uniformly distributed in the detector and in energy over $E_{dep} \in [1, 10]$ MeV. The training dataset account for 990'000 events with 10'000 events reserved for validation. The data are normalized, mean shifted to 0 and standard deviation scaled to 1, before being processed by the network.</div><div class="clear"></div>
<div class="linenb">328</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">329</div><div class="codeline">Each epoch go through the entire training datasets, with a batch size of 64. The training last for 25 epochs. The performance the FFNN are presented in Figures \ref{fig:janne:ffnn:ESB} and \ref{fig:janne:ffnn:SB}. We remind that goal of this FFNN is not to have competitive performances against classical algorithms like OMILREC but more to have a simple, NN reconstruction algorithm to run the ANN against.</div><div class="clear"></div>
<div class="linenb">330</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">331</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[ht]</div><div class="clear"></div>
<div class="linenb">332</div><div class="codeline">&nbsp;&nbsp;\centering</div><div class="clear"></div>
<div class="linenb">333</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\begin{subfigure}</span>[t]{0.48\linewidth}</div><div class="clear"></div>
<div class="linenb">334</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[width=\linewidth]{images/janne/ffnn/ESBE.png}</div><div class="clear"></div>
<div class="linenb">335</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\end{subfigure}</span></div><div class="clear"></div>
<div class="linenb">336</div><div class="codeline">&nbsp;&nbsp;\hfill</div><div class="clear"></div>
<div class="linenb">337</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\begin{subfigure}</span>[t]{0.48\linewidth}</div><div class="clear"></div>
<div class="linenb">338</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[width=\linewidth]{images/janne/ffnn/ESBR.png}</div><div class="clear"></div>
<div class="linenb">339</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\end{subfigure}</span></div><div class="clear"></div>
<div class="linenb">340</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\caption</span>{Energy resolution of the FFNN with respect to the energy (<span class="keyword1">\textbf</span>{On the left}) and with respect to the radius (<span class="keyword1">\textbf</span>{On the right}).}</div><div class="clear"></div>
<div class="linenb">341</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{fig:janne:ffnn:ESB}</div><div class="clear"></div>
<div class="linenb">342</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">343</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">344</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[ht]</div><div class="clear"></div>
<div class="linenb">345</div><div class="codeline">&nbsp;&nbsp;\centering</div><div class="clear"></div>
<div class="linenb">346</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\begin{subfigure}</span>[t]{0.48\linewidth}</div><div class="clear"></div>
<div class="linenb">347</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[width=\linewidth]{images/janne/ffnn/SBE.png}</div><div class="clear"></div>
<div class="linenb">348</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\end{subfigure}</span></div><div class="clear"></div>
<div class="linenb">349</div><div class="codeline">&nbsp;&nbsp;\hfill</div><div class="clear"></div>
<div class="linenb">350</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\begin{subfigure}</span>[t]{0.48\linewidth}</div><div class="clear"></div>
<div class="linenb">351</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[width=\linewidth]{images/janne/ffnn/SBR.png}</div><div class="clear"></div>
<div class="linenb">352</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\end{subfigure}</span></div><div class="clear"></div>
<div class="linenb">353</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\caption</span>{Radial resolution of the FFNN with respect to the energy (<span class="keyword1">\textbf</span>{On the left}) and with respect to the radius (<span class="keyword1">\textbf</span>{On the right}).}</div><div class="clear"></div>
<div class="linenb">354</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{fig:janne:ffnn:SB}</div><div class="clear"></div>
<div class="linenb">355</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">356</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">357</div><div class="codeline"><span class="keyword1">\subsection</span>{Adversarial Neural Network (ANN)}</div><div class="clear"></div>
<div class="linenb">358</div><div class="codeline"><span class="keyword1">\label</span>{sec:janne:arch:ann}</div><div class="clear"></div>
<div class="linenb">359</div><div class="codeline"><span class="comment"><span class="comment">%<span class="keyword2">\begin{itemize}</span></span></span></div><div class="clear"></div>
<div class="linenb">360</div><div class="codeline"><span class="comment"><span class="comment">%  <span class="keyword1">\item</span> Decrire l'architecture de l'ANN</span></span></div><div class="clear"></div>
<div class="linenb">361</div><div class="codeline"><span class="comment"><span class="comment">%<span class="keyword2">\end{itemize}</span></span></span></div><div class="clear"></div>
<div class="linenb">362</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">363</div><div class="codeline">The ANN aims to introduce perturbations in the event data in such a way that these perturbations are not detectable in the control dataset while still degrading the energy reconstruction of the IBD dataset. For this purpose, and for the reasons detailed in Section \ref{sec:janne:back_prop}, the ANN operates on the inputs of the reconstruction network presented above, namely the FFNN. During the training, the parameters of the FFNN are <span class="keyword1">\textit</span>{frozen,} meaning they will not be updated during the ANN training. If they were free to be optimized, they would adapt to the perturbations of the ANN, which would go against the objective of this work.</div><div class="clear"></div>
<div class="linenb">364</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">365</div><div class="codeline">The FFNN takes as input a vector of $5 \times 3072$ values, representing the $(x,y,z,Q,t)$ of 3072 HealPix pixels (pixelization illustrated in Figure \ref{fig:jgnn:healpix}). Those values come from the aggregation of the PMTs belonging to those pixels.</div><div class="clear"></div>
<div class="linenb">366</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">367</div><div class="codeline">It seems unreasonable that the ANN would modify the HealPix pixel positions, as they are derived from a mathematical construction. It could, however, perturb which PMTs are assigned to specific pixels, introducing localization errors, but the position of the PMTs is carefully monitored during JUNO's construction. Such aggregation errors would likely arise from PMTs located at the edges of the pixels, yet this scenario seems unlikely. Moreover, due to the constraints mentioned in Section \ref{sec:janne:back_prop}, the ANN is required to work with the same format that the FFNN uses as input.</div><div class="clear"></div>
<div class="linenb">368</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">369</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">370</div><div class="codeline">At the start of the project, we attempted to have it operate on both time and charge information simultaneously, but it struggled to converge. After discussions with colleagues in the collaboration, we decided that the ANN would only introduce perturbations in the charge information, as most of the energy information comes from the charge.</div><div class="clear"></div>
<div class="linenb">371</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">372</div><div class="codeline">Our ANN thus needs to output a 3072-dimensional vector, which represents the updated charges of the detector.</div><div class="clear"></div>
<div class="linenb">373</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">374</div><div class="codeline">We decided on a Fully Connected Deep NN (DNN) ``bottleneck'' architecture for the ANN, illustrated in Figure \ref{fig:janne:ann_arch}. This architecture places a 4096-neuron-wide layer after the input, followed by smaller layers of sizes 2048, 1024, and 512 neurons, before finally reaching the 256-neuron layer. From this layer, the size increases again to 512, 1024, and finally 2048 neurons before the output layer, which consists of 3072 neurons.</div><div class="clear"></div>
<div class="linenb">375</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">376</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">377</div><div class="codeline"><span class="keyword2">\begin{figure}</span></div><div class="clear"></div>
<div class="linenb">378</div><div class="codeline">&nbsp;&nbsp;\centering</div><div class="clear"></div>
<div class="linenb">379</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[height=6cm]{images/janne/ANN_illustration.png}</div><div class="clear"></div>
<div class="linenb">380</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\caption</span>{Illustration of the ``bottleneck'' architecture of the ANN. Each block represent a fully connected layer with, on the left, the input layer and on the right the output layer. We see a first reduction of the number of neurons per layer, going from 4096 to 256, followed by an augmentation back to 4096 neurons, thus the ``bottleneck''.}</div><div class="clear"></div>
<div class="linenb">381</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{fig:janne:ann_arch}</div><div class="clear"></div>
<div class="linenb">382</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">383</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">384</div><div class="codeline">The idea behind this architecture is that, by reducing the number of neurons per layer, we force the network to summarize the event in 256 parameters, that it will use to regenerate an event. This architecture has also the advantage of keeping the number of learnable parameters relatively small, as the connection between small layers do not require a lot of parameters.</div><div class="clear"></div>
<div class="linenb">385</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">386</div><div class="codeline"><span class="keyword1">\subsubsection</span>{ANN loss}</div><div class="clear"></div>
<div class="linenb">387</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">388</div><div class="codeline">As it was mentioned in the introduction of Section \ref{sec:janne:arch}, the loss of the ANN is composed of two losses, the adversarial loss ${\cal L}_{adv}$ and the regularization loss ${\cal L}_{reg}$. To those two losses, we adjoin a penalty term that prevent the ANN from producing non-physical events.</div><div class="clear"></div>
<div class="linenb">389</div><div class="codeline"><span class="keyword2">\begin{equation*}</span></div><div class="clear"></div>
<div class="linenb">390</div><div class="codeline">&nbsp;&nbsp;{\cal L} = {\cal L}_{adv} + {\cal L}_{reg} + P</div><div class="clear"></div>
<div class="linenb">391</div><div class="codeline"><span class="keyword2">\end{equation*}</span></div><div class="clear"></div>
<div class="linenb">392</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">393</div><div class="codeline">The adversarial loss ${\cal L}_{adv}$ is defined as the absolute value correlation between the reconstructed energy and the energy deposit (Eq. \ref{eq:janne:ladv}). The regularization loss ${\cal L}_{reg}$ is the MSE of the true and reconstructed energy position vector $(x, y, z, E)$ (Eq. \ref{eq:janne:lreg}).</div><div class="clear"></div>
<div class="linenb">394</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">395</div><div class="codeline">The penalty term is here to prevent the network from generating event that are too far from the initial event. The relevance of this term and its parameters will be further discussed in Section \ref{sec:janne:arch:training}. The penalty $P$ is a function that takes the pixelated event $X$, its transformation after the ANN ${\cal G}(X)$ and a constraint $\epsilon$</div><div class="clear"></div>
<div class="linenb">396</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">397</div><div class="codeline">&nbsp;&nbsp;P(X, {\cal G}(X), \epsilon) = \sum_{i=1}^{3072} \left( ReLU(-{\cal G}(X)_i) + D_i \right)</div><div class="clear"></div>
<div class="linenb">398</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">399</div><div class="codeline">with</div><div class="clear"></div>
<div class="linenb">400</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">401</div><div class="codeline">&nbsp;&nbsp;D_i = <span class="keyword2">\begin{cases}</span></div><div class="clear"></div>
<div class="linenb">402</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;\frac{(X_i - {\cal G}(X)_i)^2}{X_i^2}&amp; \text{if } \frac{|X_i - {\cal G}(X)_i|}{X_i} &gt; \epsilon \\</div><div class="clear"></div>
<div class="linenb">403</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;0&amp; \text{otherwise}</div><div class="clear"></div>
<div class="linenb">404</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\end{cases}</span></div><div class="clear"></div>
<div class="linenb">405</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">406</div><div class="codeline">where<span class="highlight" title="The personal pronoun 'I' should be uppercase.. Suggestions: [I] (28235) [lt:en:I_LOWERCASE]"> $i$</span> index the HealPix pixels. The term $ReLU(-{\cal G}(X)_i$ is minimal, equal 0, when the charge after perturbation is positive. This term prevents the ANN from producing negative charge, feat impossible for the PMTs.</div><div class="clear"></div>
<div class="linenb">407</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">408</div><div class="codeline">The second term $D_i$ is equal to 0 when the relative charge between the original and perturbed pixel is less than $\epsilon$. The value of $\epsilon$ will change during the training, as it will be explained in Section \ref{sec:janne:arch:training}. Otherwise, it is the square of this relative charge difference. This term <span class="highlight" title="The verb 'penalize' is plural. Did you mean: 'penalizes'? Did you use a verb instead of a noun?. Suggestions: [penalizes] (28707) [lt:en:PLURAL_VERB_AFTER_THIS]">penalize</span> the ANN from producing charges too different from the original event.</div><div class="clear"></div>
<div class="linenb">409</div><div class="codeline">\hfill</div><div class="clear"></div>
<div class="linenb">410</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">411</div><div class="codeline">When dealing with multiple losses like this, it is important keep then of the same order of magnitude, as we do not want one term to absorb the other.</div><div class="clear"></div>
<div class="linenb">412</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">413</div><div class="codeline">The loss ${\cal L}_{adv}$ range from 0 to 1 while ${\cal L}_{reg}$ is 0 when the vertex and energy is perfectly reconstructed. It can theoretically go up to infinity. In practice, we expect it to take value of the order of magnitude coherent with the reconstruction performances. In fact, if it would take higher value, it would mean that the reconstruction would reconstruct the event far away from the true vertex and energy in comparison to the expected performance. This kind of issue would be immediately be detected, even with simplistic reconstructions such as the charge barycenter, which goes against the goal of producing subtle fluctuation.</div><div class="clear"></div>
<div class="linenb">414</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">415</div><div class="codeline">We evaluate ${\cal L}_{reg}$ with $(x, y, z)$ in meter and $E$ in MeV. If the event is reconstructed with a precision of 15 cm and an energy resolution of 3\% at 1 MeV, taking the reconstruction performance of the best reconstruction algorithm OMILREC (see Sections \ref{sec:juno:reco} and \ref{sec:jgnn:results}), ${\cal L}_{reg} \approx 0.3^2 + 0.03^2 = 0.0909$. We see about an order of magnitude between ${\cal L}_{adv}$ and ${\cal L}_{reg}$. To compensate for it, we weight ${\cal L}_{reg}$</div><div class="clear"></div>
<div class="linenb">416</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">417</div><div class="codeline">&nbsp;&nbsp;{\cal L} = {\cal L}_{adv} + 60\cdot{\cal L}_{reg} + P(\epsilon)</div><div class="clear"></div>
<div class="linenb">418</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">419</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">420</div><div class="codeline">The amplitude of $P$ and the value of $\epsilon$ will be further discussed in Section \ref{sec:janne:arch:training}.</div><div class="clear"></div>
<div class="linenb">421</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">422</div><div class="codeline">\<span class="highlight-sh" title="This subsubsection is very short (about 137 words). You should consider merging it with another section or make it longer. [sh:seclen]">subsubsection</span>{Hyperparameter optimization}</div><div class="clear"></div>
<div class="linenb">423</div><div class="codeline"><span class="comment"><span class="comment">%<span class="keyword1">\label</span>{sec:janne:arch:hyper}</span></span></div><div class="clear"></div>
<div class="linenb">424</div><div class="codeline"><span class="comment"><span class="comment">%<span class="keyword2">\begin{itemize}</span></span></span></div><div class="clear"></div>
<div class="linenb">425</div><div class="codeline"><span class="comment"><span class="comment">%  <span class="keyword1">\item</span> Pour les meme raison que l'ANN:</span></span></div><div class="clear"></div>
<div class="linenb">426</div><div class="codeline"><span class="comment"><span class="comment">%    <span class="keyword2">\begin{itemize}</span></span></span></div><div class="clear"></div>
<div class="linenb">427</div><div class="codeline"><span class="comment"><span class="comment">%      <span class="keyword1">\item</span> Phase exploratoire, architecture tres changeante, random search n'est pas viable</span></span></div><div class="clear"></div>
<div class="linenb">428</div><div class="codeline"><span class="comment"><span class="comment">%      <span class="keyword1">\item</span> Architecture consomme beaucoup, besoin d'entrainer sur l'A100</span></span></div><div class="clear"></div>
<div class="linenb">429</div><div class="codeline"><span class="comment"><span class="comment">%      <span class="keyword1">\item</span> Possiblement que de l'optimization permetterais de faire passer sur V100, mais developement techniques necessaires.</span></span></div><div class="clear"></div>
<div class="linenb">430</div><div class="codeline"><span class="comment"><span class="comment">%    <span class="keyword2">\end{itemize}</span></span></span></div><div class="clear"></div>
<div class="linenb">431</div><div class="codeline"><span class="comment"><span class="comment">%<span class="keyword2">\end{itemize}</span></span></span></div><div class="clear"></div>
<div class="linenb">432</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">433</div><div class="codeline">All the ANN hyperparameters presented above have been optimized through the numerous iteration the architecture went through. The training is computationally expensive as we need to host both networks on the GPU card, reaching quickly the memory limit of the GPU. The training of the ANN can take up to 90 h. The requirement of having a powerful GPU can be met locally, as Subatech possess an available A100 \cite{noauthor_nvidia_nodate-1} card with 40 GB of memory. We could not port over computing center as they only possess V100 \cite{noauthor_nvidia_nodate-2} GPU with 20 GB of memory.</div><div class="clear"></div>
<div class="linenb">434</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">435</div><div class="codeline">Those constraints made a random search optimization impossible. It is maybe possible, through optimization, to reduce the memory requirements to reach the threshold to run on V100<span class="highlight" title="Use a comma before 'but' if it connects two independent clauses (unless they are closely connected and short).. Suggestions: [, but] (30731) [lt:en:COMMA_COMPOUND_SENTENCE_2]"> but</span> the challenge was deemed not worth it for an exploratory work.</div><div class="clear"></div>
<div class="linenb">436</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">437</div><div class="codeline"><span class="keyword1">\subsection</span>{Training of the ANN}</div><div class="clear"></div>
<div class="linenb">438</div><div class="codeline"><span class="keyword1">\label</span>{sec:janne:arch:training}</div><div class="clear"></div>
<div class="linenb">439</div><div class="codeline"><span class="comment"><span class="comment">%<span class="keyword2">\begin{itemize}</span></span></span></div><div class="clear"></div>
<div class="linenb">440</div><div class="codeline"><span class="comment"><span class="comment">%  <span class="keyword1">\item</span> Presentation du dataset</span></span></div><div class="clear"></div>
<div class="linenb">441</div><div class="codeline"><span class="comment"><span class="comment">%  <span class="keyword1">\item</span> 2 etapes d'entrainement</span></span></div><div class="clear"></div>
<div class="linenb">442</div><div class="codeline"><span class="comment"><span class="comment">%  <span class="keyword1">\item</span> Retour à l'identitié -&gt; que l'ANN ne fasse pas n'importe quoi</span></span></div><div class="clear"></div>
<div class="linenb">443</div><div class="codeline"><span class="comment"><span class="comment">%  <span class="keyword1">\item</span> Cassage de la reconstruction</span></span></div><div class="clear"></div>
<div class="linenb">444</div><div class="codeline"><span class="comment"><span class="comment">%<span class="keyword2">\end{itemize}</span></span></span></div><div class="clear"></div>
<div class="linenb">445</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">446</div><div class="codeline">The ANN training is divided into two phases. In the first phase, the network learns to accurately reproduce physical events, ensuring that it can handle the intrinsic variability of the detector's response. This step is crucial, as it provides the foundation for the second phase, where the network searches for subtle perturbations that can degrade the reconstruction without being detected by standard calibration procedures. Splitting the training into two phases also allow saving a version of the network that know how to reproduce the physical events. We can then ``resume'' the training from this point if we just introduce changes in Phase 2, saving the training time of Phase 1.</div><div class="clear"></div>
<div class="linenb">447</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">448</div><div class="codeline">For both phases, we use the both of the datasets presented in Section \ref{sec:janne:method}. We use a batch size of 64 for both datasets meaning that, for each step, the network see 128 events.</div><div class="clear"></div>
<div class="linenb">449</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">450</div><div class="codeline">Each epoch go through the entirety of the training dataset.</div><div class="clear"></div>
<div class="linenb">451</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">452</div><div class="codeline"><span class="keyword1">\subsubsection</span>{First training phase: back to physics}</div><div class="clear"></div>
<div class="linenb">453</div><div class="codeline"><span class="keyword1">\label</span>{sec:janne:results:identity}</div><div class="clear"></div>
<div class="linenb">454</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">455</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[ht]</div><div class="clear"></div>
<div class="linenb">456</div><div class="codeline">&nbsp;&nbsp;\centering</div><div class="clear"></div>
<div class="linenb">457</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[height=6cm]{images/janne/training/phase_1_penal.png}</div><div class="clear"></div>
<div class="linenb">458</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\caption</span>{Evolution of the loss ${\cal L}_1 = 0.25 \cdot P(0.01)$ during the first phase of the training.}</div><div class="clear"></div>
<div class="linenb">459</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{fig:jann:train:phase_1}</div><div class="clear"></div>
<div class="linenb">460</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">461</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">462</div><div class="codeline">When the ANN is initialized, before any training has been done, its parameters are initialized with random values. Multiple initialization methods exist. In this work, we use a common <span class="highlight" title="Check that the noun 'initialization' after the pronoun 'He' is correct. It's possible that you may need to switch to a possessive pronoun, or use another part of speech. (31972) [lt:en:PRP_VB]">He initialization</span> \cite{he_delving_2015}, which is the default initialization in the PyTorch \cite{ansel_pytorch_2024} library. If we were to ask for an event from the ANN without training first, the results would be random noise. We thus first have the ANN learn to reproduce physical events.</div><div class="clear"></div>
<div class="linenb">463</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">464</div><div class="codeline">For this, we conduct a training of 200 epochs where the loss consists only of the penalty term. For scaling purposes, the penalty $P$ is scaled by 0.25.</div><div class="clear"></div>
<div class="linenb">465</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">466</div><div class="codeline">&nbsp;&nbsp;{\cal L}_1 = 0.25 \cdot P(\epsilon = 0.01)</div><div class="clear"></div>
<div class="linenb">467</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">468</div><div class="codeline">During this phase, the only objective of the network is to yield events that are the same as the original events.</div><div class="clear"></div>
<div class="linenb">469</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">470</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">471</div><div class="codeline">The evolution of this loss ${\cal L}_1$ during the training for the training dataset and the validation dataset is presented in Figure \ref{fig:jann:train:phase_1}.</div><div class="clear"></div>
<div class="linenb">472</div><div class="codeline">We see that the ANN converges to some stability in the loss.</div><div class="clear"></div>
<div class="linenb">473</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">474</div><div class="codeline">The time and charge channels of two events, after this training phase, are presented in Figures \ref{fig:janne:hr_he_200} and \ref{fig:janne:lr_le_200}. We remind that the ANN only act on the charge channel of the event.</div><div class="clear"></div>
<div class="linenb">475</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[!ht]</div><div class="clear"></div>
<div class="linenb">476</div><div class="codeline">&nbsp;&nbsp;\centering</div><div class="clear"></div>
<div class="linenb">477</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[height=7cm]{images/janne/events/hr_he_200.png}</div><div class="clear"></div>
<div class="linenb">478</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\caption</span>{Time channel (on the left) and charge channel (on the right) of a <span class="keyword1">\textbf</span>{radial, high energy event} ($R$ = 17.2 m, $E_{dep}$ = 7.1 MeV), <span class="keyword1">\textbf</span>{Top:} before the ANN perturbation, <span class="keyword1">\textbf</span>{Bottom:} after the ANN perturbation. The ANN have been trained for 200 epochs, just after Phase 1. Time channel in ns and charge channel in $N_{pe}$.}</div><div class="clear"></div>
<div class="linenb">479</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{fig:janne:hr_he_200}</div><div class="clear"></div>
<div class="linenb">480</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">481</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">482</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[!ht]</div><div class="clear"></div>
<div class="linenb">483</div><div class="codeline">&nbsp;&nbsp;\centering</div><div class="clear"></div>
<div class="linenb">484</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[height=7cm]{images/janne/events/lr_le_200.png}</div><div class="clear"></div>
<div class="linenb">485</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\caption</span>{Time channel (on the left) and charge channel (on the right) of a <span class="keyword1">\textbf</span>{central, low energy event} ($R$ = 9.1 m, $E_{dep}$ = 1.9 MeV), <span class="keyword1">\textbf</span>{Top:} before the ANN perturbation, <span class="keyword1">\textbf</span>{Bottom:} after the ANN perturbation. The ANN have been trained for 200 epochs, just after Phase 1. Time channel in ns and charge channel in $N_{pe}$.}</div><div class="clear"></div>
<div class="linenb">486</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{fig:janne:lr_le_200}</div><div class="clear"></div>
<div class="linenb">487</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">488</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">489</div><div class="codeline">We observe that for a localized event, Figure \ref{fig:janne:hr_he_200}, the ANN correctly reproduces the event, while for a more diffuse event, Figure \ref{fig:janne:lr_le_200}, it produces a more uniform charge distribution. By looking at the color scale in Figure \ref{fig:janne:lr_le_200}, we observe that the ANN does not reproduce singular high numbers of $N_{pe}$. The highest pixel in the original was 12 $N_{pe}$, whereas after the ANN, the highest pixel is 5 $N_{pe}$. Furthermore, whereas in the original event the charge distribution, while diffuse, was still concentrated in specific pixels, the ANN spreads the charges in all the pixels.</div><div class="clear"></div>
<div class="linenb">490</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">491</div><div class="codeline">In the next figures, we discuss the reconstruction of the FFNN (${\cal F}$) with and without the presence of the ANN $({\cal G})$ at the end of this Phase 1. The reconstruction by the FFNN of an event perturbed by the ANN is denoted $({\cal F} \circ {\cal G})$. We differentiate the reconstruction between the two datasets, presented in Section \ref{sec:janne:method}: the physics dataset, designated as IBD, and the control dataset, designated $^{12}B$.</div><div class="clear"></div>
<div class="linenb">492</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">493</div><div class="codeline">In Figure \ref{fig:janne:f_circ_over_f_200}, we show the ratio between the reconstructed energy distribution before and after the application of the ANN. For the $^{12}$B dataset, the ratio is close to one except in the bin $E_{rec} &gt; 9.5$ MeV, where we see an excess of events after the ANN. For the IBD dataset, the ratio is close to 1 over the energy range.</div><div class="clear"></div>
<div class="linenb">494</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">495</div><div class="codeline">In Figure \ref{fig:janne:rec_err_200}, we present the distribution of the relative reconstruction errors $(E_{rec}, E_{dep})/E_{dep}$ with (light histogram) and without (dark histogram) the perturbations predicted by the ANN. We see that without the ANN, the distribution was centered on 0, whereas with it, we observe a small positive bias. In the second row of the histogram, the ratio between the light and dark histograms, we see confirmation of the previous observation, with a deficit of events for $-0.05 &lt; (E_{rec}, E_{dep})/E_{dep}) &lt; 0.02$ and an excess of events for $(E_{rec}, E_{dep})/E_{dep}) &gt; 0.02$. This shift to higher energy explains the excess of events seen in the highest energy bins in Fig. \ref{fig:janne:f_circ_over_f_200}. The behavior between the $^{12}$B dataset (green histogram) and the IBD dataset (blue histogram) is similar.</div><div class="clear"></div>
<div class="linenb">496</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">497</div><div class="codeline">At the end of this first reconstruction phase it's apparent that this exploratory ANN is not able to correctly reconstruct the event. This could come from the bottleneck architecture: the reduction of the event to 256 parameters is not enough to correctly rebuild the event. This subject will be further discussed in the conclusion of this chapter in Section \ref{sec:janne:conclusion}.</div><div class="clear"></div>
<div class="linenb">498</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">499</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[!ht]</div><div class="clear"></div>
<div class="linenb">500</div><div class="codeline">&nbsp;&nbsp;\centering</div><div class="clear"></div>
<div class="linenb">501</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[height=4cm]{images/janne/f_circ_over_f_200.png}</div><div class="clear"></div>
<div class="linenb">502</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\caption</span>{Ratio of the reconstructed energy spectra between $({\cal F} \circ {\cal G})$ and ${\cal F}$ at then end of Phase 1 of the training. <span class="keyword1">\textbf</span>{On the left :} For the $^{12}$B dataset. <span class="keyword1">\textbf</span>{On the right :} For the IBD dataset.}</div><div class="clear"></div>
<div class="linenb">503</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{fig:janne:f_circ_over_f_200}</div><div class="clear"></div>
<div class="linenb">504</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">505</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">506</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[!ht]</div><div class="clear"></div>
<div class="linenb">507</div><div class="codeline">&nbsp;&nbsp;\centering</div><div class="clear"></div>
<div class="linenb">508</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[height=8cm]{images/janne/rec_err_200.png}</div><div class="clear"></div>
<div class="linenb">509</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\caption</span>{<span class="keyword1">\textbf</span>{On the top :} Distribution of the relative energy reconstruction error between ${\cal F}$ (light histogram) and $({\cal F} \circ {\cal G})$ (dark histogram) at then end of Phase 1 of the training. <span class="keyword1">\textbf</span>{On the bottom :} Ratio between the light and dark histogram of the top figure.}</div><div class="clear"></div>
<div class="linenb">510</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{fig:janne:rec_err_200}</div><div class="clear"></div>
<div class="linenb">511</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">512</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">513</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">514</div><div class="codeline"><span class="keyword1">\subsubsection</span>{Second training phase: Breaking of the reconstruction}</div><div class="clear"></div>
<div class="linenb">515</div><div class="codeline"><span class="keyword1">\label</span>{sec:janne:results:break}</div><div class="clear"></div>
<div class="linenb">516</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">517</div><div class="codeline">Once the ANN is able to reproduce physical events, we change the loss so that it starts to search for potential perturbations.</div><div class="clear"></div>
<div class="linenb">518</div><div class="codeline">For this we introduce the term ${\cal L}_{adv}$ and ${\cal L}_{red}$ producing a second loss ${\cal L}_2$.</div><div class="clear"></div>
<div class="linenb">519</div><div class="codeline">Adding those terms will significantly change the loss.</div><div class="clear"></div>
<div class="linenb">520</div><div class="codeline">The previous minima in the parameter phase space the ANN found minimizing ${\cal L}_1$ will not be the minima ${\cal L}_2$. To prevent a gradient explosion, we introduce a growing factor $\lambda$ in front of the term ${\cal L}_{adv}$ and ${\cal L}_{red}$. This factor starts at $\lambda  = 0.01$ at epoch 201 and grows $\lambda_{i+1} = \lambda_{i} + 0.01$ where<span class="highlight" title="The personal pronoun 'I' should be uppercase.. Suggestions: [I] (35721) [lt:en:I_LOWERCASE]"> $i$</span> indexes the epoch. It caps at $\lambda_{max} = 1$ at epoch 300 after which it stops growing.</div><div class="clear"></div>
<div class="linenb">521</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">522</div><div class="codeline">Also, to ease the task of the ANN, we relax the constraint in the penalty term $P$ from $P(0.01)$ to $P(0.15)$.</div><div class="clear"></div>
<div class="linenb">523</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">524</div><div class="codeline">The expression of the phase 2 loss ${\cal L}_2$ becomes:</div><div class="clear"></div>
<div class="linenb">525</div><div class="codeline"><span class="keyword2">\begin{equation}</span></div><div class="clear"></div>
<div class="linenb">526</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{eq:janne:phase_2:loss}</div><div class="clear"></div>
<div class="linenb">527</div><div class="codeline">&nbsp;&nbsp;{\cal L}_2 = \lambda \left( {\cal L}_{adv} + 60 \cdot {\cal L}_{reg} \right) + 0.25 \cdot P(0.15)</div><div class="clear"></div>
<div class="linenb">528</div><div class="codeline"><span class="keyword2">\end{equation}</span></div><div class="clear"></div>
<div class="linenb">529</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">530</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">531</div><div class="codeline">This second phase of the training last for 200 more epochs, up to epoch 400.</div><div class="clear"></div>
<div class="linenb">532</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">533</div><div class="codeline">The profiles of ${\cal L}_2$, ${\cal L}_{adv}$, $60 \cdot {\cal L}_{reg}$ and $0.25 \cdot P(0.15)$ during this second phase of the training are presented in Figures \ref{fig:janne:phase_2_1} and \ref{fig:janne:phase_2_2}. The profile of the loss ${\cal L}$ over entirety of the training is presented in <span class="highlight-sh" title="Use a capital letter when referring to a specific figure: 'Figure X' [sh:figmag]">figure \ref</span>{fig:janne:phase_all}.</div><div class="clear"></div>
<div class="linenb">534</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">535</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[ht]</div><div class="clear"></div>
<div class="linenb">536</div><div class="codeline">&nbsp;&nbsp;\centering</div><div class="clear"></div>
<div class="linenb">537</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\begin{subfigure}</span>[t]{0.48\linewidth}</div><div class="clear"></div>
<div class="linenb">538</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[width=\linewidth]{images/janne/training/phase_2_loss.png}</div><div class="clear"></div>
<div class="linenb">539</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\end{subfigure}</span></div><div class="clear"></div>
<div class="linenb">540</div><div class="codeline">&nbsp;&nbsp;\hfill</div><div class="clear"></div>
<div class="linenb">541</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\begin{subfigure}</span>[t]{0.48\linewidth}</div><div class="clear"></div>
<div class="linenb">542</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[width=\linewidth]{images/janne/training/phase_2_loss_adv.png}</div><div class="clear"></div>
<div class="linenb">543</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\end{subfigure}</span></div><div class="clear"></div>
<div class="linenb">544</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\caption</span>{Profile of the loss ${\cal L}_2$ and ${\cal L}_{adv}$ during the second phase of training. The linear increase of ${\cal L}_2$ is due to the growing factor $\lambda$ in Eq. \ref{eq:janne:phase_2:loss}.}</div><div class="clear"></div>
<div class="linenb">545</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{fig:janne:phase_2_1}</div><div class="clear"></div>
<div class="linenb">546</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">547</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">548</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[ht]</div><div class="clear"></div>
<div class="linenb">549</div><div class="codeline">&nbsp;&nbsp;\centering</div><div class="clear"></div>
<div class="linenb">550</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\begin{subfigure}</span>[t]{0.48\linewidth}</div><div class="clear"></div>
<div class="linenb">551</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[width=\linewidth]{images/janne/training/phase_2_loss_reg.png}</div><div class="clear"></div>
<div class="linenb">552</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\end{subfigure}</span></div><div class="clear"></div>
<div class="linenb">553</div><div class="codeline">&nbsp;&nbsp;\hfill</div><div class="clear"></div>
<div class="linenb">554</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\begin{subfigure}</span>[t]{0.48\linewidth}</div><div class="clear"></div>
<div class="linenb">555</div><div class="codeline">&nbsp;&nbsp;&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[width=\linewidth]{images/janne/training/phase_2_penal.png}</div><div class="clear"></div>
<div class="linenb">556</div><div class="codeline">&nbsp;&nbsp;<span class="keyword2">\end{subfigure}</span></div><div class="clear"></div>
<div class="linenb">557</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\caption</span>{Profile of the loss $60 \cdot {\cal L}_{reg}$ and $0.25 \cdot P(0.15)$ during the second phase of training.}</div><div class="clear"></div>
<div class="linenb">558</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{fig:janne:phase_2_2}</div><div class="clear"></div>
<div class="linenb">559</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">560</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">561</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[ht]</div><div class="clear"></div>
<div class="linenb">562</div><div class="codeline">&nbsp;&nbsp;\centering</div><div class="clear"></div>
<div class="linenb">563</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[height=6cm]{images/janne/training/phase_all_l.png}</div><div class="clear"></div>
<div class="linenb">564</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\caption</span>{Profile of the loss over the entirety of the training (Phase 1 and 2).}</div><div class="clear"></div>
<div class="linenb">565</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{fig:janne:phase_all}</div><div class="clear"></div>
<div class="linenb">566</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">567</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">568</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">569</div><div class="codeline">We see on Figures \ref{fig:janne:phase_2_1} and \ref{fig:janne:phase_2_2} that during the first epochs of this second phase, $L_{reg}$ and $P(0.15)$ decrease fast. At the end of the 1st phase, the</div><div class="clear"></div>
<div class="linenb">570</div><div class="codeline">work to recover the initial reconstruction seems incomplete since the performance of FFNN were not recovered (Fig. \ref{fig:janne:rec_err_200}). During the first epochs of the second phase, $L_{reg}$ seems to continue this work. This is logical since this term is suppose to temperate the perturbations<span class="highlight" title="Use a comma before 'so' if it connects two independent clauses (unless they are closely connected and short).. Suggestions: [, so] (36613) [lt:en:COMMA_COMPOUND_SENTENCE]"> so</span> they are not visible with a control sample.  At the same time, we see a quick increase of $L_{adv}$, confirming that the reconstruction is less broken.</div><div class="clear"></div>
<div class="linenb">571</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">572</div><div class="codeline">During most of the next 50 epochs of this phase, all losses are more stable before $L_{adv}$ starts decreasing, between epochs 250 and 300. It suggests $L_{adv}$  has managed to re-deteriorate the reconstruction, despite the quasi stability  (or slow decrease) of $L_{reg}$. This is the desired behavior concerning the losses.</div><div class="clear"></div>
<div class="linenb">573</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">574</div><div class="codeline">Unfortunately, as can be seen in Figs. \ref{fig:janne:f_circ_over_f_400} and \ref{fig:janne:rec_err_200}, the performance of the reconstruction is still too deteriorated: it would very probably be detected by data/MC comparisons with control samples. Also, on these figures and on Figure \ref{fig:janne:ann_effect_400}, we can't see an indication that IBD events are affected more than $^{12}B$ events by the perturbation. A difference here is not mandatory: the same deterioration of the resolution could still be undetectable in data/MC comparison of the distributions of the energy or of the vertex position  in $^{12}B$ samples, and still be enough to alter the oscillation analysis. However, observing a difference here would have been a good sign.</div><div class="clear"></div>
<div class="linenb">575</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">576</div><div class="codeline">After 200 epochs of Phase 2, the correlation in ${\cal L}_{adv}$ is still at 0.998, the penalty term $P(0.01)$ is stable and the regularization loss ${\cal L}_{reg}$ is close to stability.</div><div class="clear"></div>
<div class="linenb">577</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">578</div><div class="codeline">For illustration, events produced by the ANN after 400 epochs are displayed in Figures \ref{fig:janne:hr_he_400} and \ref{fig:janne:lr_le_400}. These are the same event as displayed in Figures \ref{fig:janne:hr_he_200} and \ref{fig:janne:lr_le_200}.</div><div class="clear"></div>
<div class="linenb">579</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">580</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[!ht]</div><div class="clear"></div>
<div class="linenb">581</div><div class="codeline">&nbsp;&nbsp;\centering</div><div class="clear"></div>
<div class="linenb">582</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[height=8cm]{images/janne/events/hr_he_400.png}</div><div class="clear"></div>
<div class="linenb">583</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\caption</span>{Time channel (on the left) and charge channel (on the right) of a <span class="keyword1">\textbf</span>{radial, high energy event} ($R$ = 17.2 m, $E_{dep}$ = 7.1 MeV), <span class="keyword1">\textbf</span>{Top:} before the ANN perturbation, <span class="keyword1">\textbf</span>{Bottom:} after the ANN perturbation. The ANN have been trained for 400 epochs, just after Phase 2. Time channel in ns and charge channel in $N_{pe}$.}</div><div class="clear"></div>
<div class="linenb">584</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{fig:janne:hr_he_400}</div><div class="clear"></div>
<div class="linenb">585</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">586</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">587</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[!ht]</div><div class="clear"></div>
<div class="linenb">588</div><div class="codeline">&nbsp;&nbsp;\centering</div><div class="clear"></div>
<div class="linenb">589</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[height=8cm]{images/janne/events/lr_le_400.png}</div><div class="clear"></div>
<div class="linenb">590</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\caption</span>{Time channel (on the left) and charge channel (on the right) of a <span class="keyword1">\textbf</span>{central, low energy event} ($R$ = 9.1 m, $E_{dep}$ = 1.9 MeV), <span class="keyword1">\textbf</span>{Top:} before the ANN perturbation, <span class="keyword1">\textbf</span>{Bottom:} after the ANN perturbation. The ANN have been trained for 400 epochs, just after Phase 2. Time channel in ns and charge channel in $N_{pe}$.}</div><div class="clear"></div>
<div class="linenb">591</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{fig:janne:lr_le_400}</div><div class="clear"></div>
<div class="linenb">592</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">593</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">594</div><div class="codeline">The same observations that were made after phase 1 still apply after phase 2. The ANN still spreads the charge over multiple pixels for central events, Figure \ref{fig:janne:lr_le_400}, while for radial events it is able to reproduce the small localization of the event.</div><div class="clear"></div>
<div class="linenb">595</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">596</div><div class="codeline">When looking at the distribution of ratio between the reconstructed energy distribution before and after the application of the ANN, Figure \ref{fig:janne:f_circ_over_f_400}, we observe this time a deficit of events in the high energy bin. This deficit is explained by the comparison between the distribution of relative reconstruction errors, Figure \ref{fig:janne:rec_err_400}, in which we see a small negative bias. This same figure shows a wider loss in resolution when the ANN is present. <span class="highlight" title="Three successive sentences begin with the same word. Consider rewording the sentence or use a thesaurus to find a synonym. (38713) [lt:en:ENGLISH_WORD_REPEAT_BEGINNING_RULE]">This</span> is the ANN working to degrade the resolution of the FFNN.</div><div class="clear"></div>
<div class="linenb">597</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">598</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[!ht]</div><div class="clear"></div>
<div class="linenb">599</div><div class="codeline">&nbsp;&nbsp;\centering</div><div class="clear"></div>
<div class="linenb">600</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[height=4cm]{images/janne/f_circ_over_f_400.png}</div><div class="clear"></div>
<div class="linenb">601</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\caption</span>{Ratio of the reconstructed energy spectra between $({\cal F} \circ {\cal G})$ and ${\cal F}$ at then end of Phase 2 of the training. <span class="keyword1">\textbf</span>{On the left :} For the $^{12}$B dataset. <span class="keyword1">\textbf</span>{On the right :} For the IBD dataset.}</div><div class="clear"></div>
<div class="linenb">602</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{fig:janne:f_circ_over_f_400}</div><div class="clear"></div>
<div class="linenb">603</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">604</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">605</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[!ht]</div><div class="clear"></div>
<div class="linenb">606</div><div class="codeline">&nbsp;&nbsp;\centering</div><div class="clear"></div>
<div class="linenb">607</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[height=8cm]{images/janne/rec_err_400.png}</div><div class="clear"></div>
<div class="linenb">608</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\caption</span>{<span class="keyword1">\textbf</span>{On the top :} Distribution of the relative energy reconstruction error between ${\cal F}$ (light histogram) and $({\cal F} \circ {\cal G})$ (dark histogram) at then end of Phase 2 of the training. <span class="keyword1">\textbf</span>{On the bottom :} Ratio between the light and dark histogram of the top figure.}</div><div class="clear"></div>
<div class="linenb">609</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{fig:janne:rec_err_400}</div><div class="clear"></div>
<div class="linenb">610</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">611</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">612</div><div class="codeline"><span class="keyword2">\begin{figure}</span>[!ht]</div><div class="clear"></div>
<div class="linenb">613</div><div class="codeline">&nbsp;&nbsp;\centering</div><div class="clear"></div>
<div class="linenb">614</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\includegraphics</span>[height=4cm]{images/janne/ann_effect_400.png}</div><div class="clear"></div>
<div class="linenb">615</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\caption</span>{Ratio between the relative error on the reconstructed energy between the IBD and the $^{12}$B dataset. <span class="keyword1">\textbf</span>{On the right :} without the ANN. <span class="keyword1">\textbf</span>{On the left :} with the ANN.}</div><div class="clear"></div>
<div class="linenb">616</div><div class="codeline">&nbsp;&nbsp;<span class="keyword1">\label</span>{fig:janne:ann_effect_400}</div><div class="clear"></div>
<div class="linenb">617</div><div class="codeline"><span class="keyword2">\end{figure}</span></div><div class="clear"></div>
<div class="linenb">618</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">619</div><div class="codeline">Figure \ref{fig:janne:ann_effect_400} shows the ratio between the relative error on the reconstructed energy between the IBD and the $^{12}$B dataset with and without the ANN. We don't see any indicative difference, the ANN even seems to have harmonized the reconstruction error between the two datasets.</div><div class="clear"></div>
<div class="linenb">620</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">621</div><div class="codeline">In the next section, we will summarize the lessons we gathered while working on this ANN, as well as some perspectives for the future.</div><div class="clear"></div>
<div class="linenb">622</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">623</div><div class="codeline"><span class="keyword1">\section</span>{Conclusion and prospects}</div><div class="clear"></div>
<div class="linenb">624</div><div class="codeline"><span class="keyword1">\label</span>{sec:janne:conclusion}</div><div class="clear"></div>
<div class="linenb">625</div><div class="codeline"><span class="comment"><span class="comment">%<span class="keyword2">\begin{itemize}</span></span></span></div><div class="clear"></div>
<div class="linenb">626</div><div class="codeline"><span class="comment"><span class="comment">%  <span class="keyword1">\item</span> Not enough</span></span></div><div class="clear"></div>
<div class="linenb">627</div><div class="codeline"><span class="comment"><span class="comment">%  <span class="keyword1">\item</span> Probably guide the ANN</span></span></div><div class="clear"></div>
<div class="linenb">628</div><div class="codeline"><span class="comment"><span class="comment">%<span class="keyword2">\end{itemize}</span></span></span></div><div class="clear"></div>
<div class="linenb">629</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">630</div><div class="codeline">Reliability and knowledge of our reconstruction algorithms are crucial for the successful conduct of the experiment. The first step to testing and comparing the reconstruction algorithms is to have them publicly available. To this end, I have implemented a BDT for energy reconstruction in JUNO's common software and compared its performance and behavior in detail to the classic likelihood algorithm OMILREC. The strong correlation between their errors indicates that close to no improvement can be made by combining the two algorithms, as they use the same information.</div><div class="clear"></div>
<div class="linenb">631</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">632</div><div class="codeline">\hfill</div><div class="clear"></div>
<div class="linenb">633</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">634</div><div class="codeline">Concerning the development of an ANN, we actually developed prototype which was useful to identify several of the difficulties we will have to overcome in the future to produce an ANN fulfilling the aims defined at the beginning of this chapter. First, we determined that learning individual perturbations for each of the 17600 LPMTs (meaning more than 35000 learnable parameters if one decides to perturb $Q$ and $t$) is too much for our available hardware. Then, we determined that particular techniques would be necessary to solve the back propagation problems</div><div class="clear"></div>
<div class="linenb">635</div><div class="codeline">described in Section \ref{sec:janne:back_prop} and couple the ANN with any reconstruction algorithm pre-existing in JUNO.</div><div class="clear"></div>
<div class="linenb">636</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">637</div><div class="codeline">After having opted for a simpler prototype with 3072 learnable parameters, we faced the problem due to their random initialization and adapted by adding a new term to the loss function and splitting the training in 2 phases. Then, we experience the importance of the definition of the loss function, in particular the way to balance two antagonist terms:</div><div class="clear"></div>
<div class="linenb">638</div><div class="codeline">one (${\cal L}_{adv}$) which deteriorates the event reconstruction and one that preserves it (${\cal L}_{reg}$). The way to define each of these two terms is not trivial either. Having these notions in mind is essential before trying to develop a tool producing realist perturbation patterns at the individual PMT level.</div><div class="clear"></div>
<div class="linenb">639</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">640</div><div class="codeline">With this prototype, we manage to produce one of the desired behaviors for such an ANN: we observe ${\cal L}_{adv}$ deteriorating the reconstruction and ${\cal L}_{reg}$ reducing the deterioration. However, at the end of the training, the deterioration is too high compared to the subtle scenarios we want the ANN to produce. A solution against this could be to give a higher weight to ${\cal L}_{reg}$ or to find a more efficient penalty term $P(\epsilon)$.</div><div class="clear"></div>
<div class="linenb">641</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">642</div><div class="codeline">A smarter definition of ${\cal L}_{adv}$ could also be useful: a definition more explicitly related to our goal (biasing the oscillation analysis) might induce smaller perturbations. This could be <span class="highlight" title="Use 'an' instead of 'a' if the following word starts with a vowel sound, e.g. 'an article', 'an hour'.. Suggestions: [an] (41686) [lt:en:EN_A_VS_AN]">a</span> ${\cal L}_{adv}$ favoring a small energy dependent bias between $E_{rec}$ and $E_{dep}$ in IBD events. A smarter ${\cal L}_{adv}$ could also help to produce perturbations that affect the IBD reconstruction more than the reconstruction of $^{12}B$ or calibration events. Although this feature is not mandatory, it would be welcome. It is not achieved by the present version of the ANN.</div><div class="clear"></div>
<div class="linenb">643</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">644</div><div class="codeline">A possible explanation is the perturbations it produces seem to follow a random pattern across the 3072 pixels. It could be the result of the limited efficiency of the first phase of the training (which tries to recover from the random initialization of these perturbations), or result from the present definition of ${\cal L}_{adv}$ (which can be minimized by random perturbations).</div><div class="clear"></div>
<div class="linenb">645</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">646</div><div class="codeline">The architecture of the ANN is, for now, very simple; it's a Fully Connected Deep NN with a bottleneck architecture. Previous work in developing ML for reconstruction \cite{qian_vertex_2021} and the algorithms presented in Chapters \ref{sec:jcnn} and \ref{sec:jgnn} show the relevance of convolutions in the reconstruction, and the work of Gavrikov et al.\ \cite{gavrikov_energy_2022} presented at the beginning of this chapter hints at the importance of the time and charge distribution. A more complex and refined architecture can probably be more effective.</div><div class="clear"></div>
<div class="linenb">647</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">648</div><div class="codeline">Another architecture improvement could come from ResNet architectures \cite{he_deep_2016}. They have already proven that the introduction of residual operations helps the network reach better performance. We can imagine a network where instead of $X' = {\cal G}(X)$ we have $X' = {\cal G}(X) + X$, where the ANN ${\cal G}$ computes only the perturbation instead of a whole new event.</div><div class="clear"></div>
<div class="linenb">649</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">650</div><div class="codeline">To eventually design an ANN able to perturb several variables for each and every PMT instead of 3072 pixels, we need to find a way to reduce the number of learnable parameters. For instance, the $\delta Q$ perturbation could be a function common to all PMT, depending on learnable parameters controlling its variation against $Q$, $t$  and the position of the PMT. The choice of the function could also be guided by physics informed considerations. To help to learn perturbations affecting IBD events more than others,  the  function could also depend on the initial reconstruction of the  interaction position, since the reconstruction of annihilation gammas depends on it.</div><div class="clear"></div>
<div class="linenb">651</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">652</div><div class="codeline">Finally, to use this method on every reconstruction algorithm, we must move away from the back-propagation method, for reasons detailed in Section \ref{sec:janne:back_prop}, and use different methods such as Reinforcement Learning.</div><div class="clear"></div>
<div class="linenb">653</div><div class="codeline"></div><div class="clear"></div>
<div class="linenb">654</div><div class="codeline"><span class="keyword2">\end{document}</span></div><div class="clear"></div>
</div>
<hr/>
Output produced by TeXtidote v0.8.4, &copy; 2018-2022 Sylvain Hall&eacute; - All rights reserved.<br/>
See the <a href="https://sylvainhalle.github.io/textidote">TeXtidote website</a> for more information.
</body>
</html>
